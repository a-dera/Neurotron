{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All fingers Primary Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets and Clean\n",
    "In this configuration the relevant data set should be loaded from the same folder as the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/nfs/2018/j/jcruz-y-/neurotron_datasets/joined/joined_data_106979_24-Oct-19_17:31_jose_all_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of timestamps from the two hardware devices and a diff between them. When the two hardware data streams were stitched together an effor was made to minimize this diff, but the driver configuration did not easily permit eliminating it. This information is included to understand the accuracy of the data, but will not be used during the training.\n",
    "\n",
    "The time data is followed by the 8 channels from the Myo, this data will be used as input features.\n",
    "\n",
    "This is followed by the 63 positional points from the Leap cameras. These will be used as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Leap timestamp</th>\n",
       "      <th>timestamp diff</th>\n",
       "      <th>emg timestamp</th>\n",
       "      <th>ch1</th>\n",
       "      <th>ch2</th>\n",
       "      <th>ch3</th>\n",
       "      <th>ch4</th>\n",
       "      <th>ch5</th>\n",
       "      <th>ch6</th>\n",
       "      <th>ch7</th>\n",
       "      <th>ch8</th>\n",
       "      <th>Wrist x</th>\n",
       "      <th>Wrist y</th>\n",
       "      <th>Wrist z</th>\n",
       "      <th>Thumb Proximal x</th>\n",
       "      <th>Thumb Proximal y</th>\n",
       "      <th>Thumb Proximal z</th>\n",
       "      <th>Thumb Intermediate x</th>\n",
       "      <th>Thumb Intermediate y</th>\n",
       "      <th>Thumb Intermediate z</th>\n",
       "      <th>Thumb Distal x</th>\n",
       "      <th>Thumb Distal y</th>\n",
       "      <th>Thumb Distal z</th>\n",
       "      <th>Thumb Tip x</th>\n",
       "      <th>Thumb Tip y</th>\n",
       "      <th>Thumb Tip z</th>\n",
       "      <th>Index Proximal x</th>\n",
       "      <th>Index Proximal y</th>\n",
       "      <th>Index Proximal z</th>\n",
       "      <th>Index Intermediate x</th>\n",
       "      <th>Index Intermediate y</th>\n",
       "      <th>Index Intermediate z</th>\n",
       "      <th>Index Distal x</th>\n",
       "      <th>Index Distal y</th>\n",
       "      <th>Index Distal z</th>\n",
       "      <th>Index Tip x</th>\n",
       "      <th>Index Tip y</th>\n",
       "      <th>Index Tip z</th>\n",
       "      <th>Middle Proximal x</th>\n",
       "      <th>Middle Proximal y</th>\n",
       "      <th>Middle Proximal z</th>\n",
       "      <th>Middle Intermediate x</th>\n",
       "      <th>Middle Intermediate y</th>\n",
       "      <th>Middle Intermediate z</th>\n",
       "      <th>Middle Distal x</th>\n",
       "      <th>Middle Distal y</th>\n",
       "      <th>Middle Distal z</th>\n",
       "      <th>Middle Tip x</th>\n",
       "      <th>Middle Tip y</th>\n",
       "      <th>Middle Tip z</th>\n",
       "      <th>Ring Proximal x</th>\n",
       "      <th>Ring Proximal y</th>\n",
       "      <th>Ring Proximal z</th>\n",
       "      <th>Ring Intermediate x</th>\n",
       "      <th>Ring Intermediate y</th>\n",
       "      <th>Ring Intermediate z</th>\n",
       "      <th>Ring Distal x</th>\n",
       "      <th>Ring Distal y</th>\n",
       "      <th>Ring Distal z</th>\n",
       "      <th>Ring Tip x</th>\n",
       "      <th>Ring Tip y</th>\n",
       "      <th>Ring Tip z</th>\n",
       "      <th>Pinky Proximal x</th>\n",
       "      <th>Pinky Proximal y</th>\n",
       "      <th>Pinky Proximal z</th>\n",
       "      <th>Pinky Intermediate x</th>\n",
       "      <th>Pinky Intermediate y</th>\n",
       "      <th>Pinky Intermediate z</th>\n",
       "      <th>Pinky Distal x</th>\n",
       "      <th>Pinky Distal y</th>\n",
       "      <th>Pinky Distal z</th>\n",
       "      <th>Pinky Tip x</th>\n",
       "      <th>Pinky Tip y</th>\n",
       "      <th>Pinky Tip z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>50.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>9.262085</td>\n",
       "      <td>-9.118690</td>\n",
       "      <td>47.617905</td>\n",
       "      <td>-18.114399</td>\n",
       "      <td>-8.678528</td>\n",
       "      <td>51.673462</td>\n",
       "      <td>-46.709621</td>\n",
       "      <td>-12.567474</td>\n",
       "      <td>17.094170</td>\n",
       "      <td>-69.877258</td>\n",
       "      <td>-11.163055</td>\n",
       "      <td>-3.096992</td>\n",
       "      <td>-85.138763</td>\n",
       "      <td>-10.990265</td>\n",
       "      <td>-17.690151</td>\n",
       "      <td>-20.603283</td>\n",
       "      <td>15.033867</td>\n",
       "      <td>-19.298096</td>\n",
       "      <td>-18.594982</td>\n",
       "      <td>29.675285</td>\n",
       "      <td>-55.134228</td>\n",
       "      <td>-23.628544</td>\n",
       "      <td>21.952751</td>\n",
       "      <td>-74.898529</td>\n",
       "      <td>-29.280987</td>\n",
       "      <td>9.089638</td>\n",
       "      <td>-81.241966</td>\n",
       "      <td>-1.600807</td>\n",
       "      <td>9.199806</td>\n",
       "      <td>-21.184177</td>\n",
       "      <td>9.602978</td>\n",
       "      <td>22.597313</td>\n",
       "      <td>-61.013245</td>\n",
       "      <td>5.366776</td>\n",
       "      <td>12.613152</td>\n",
       "      <td>-84.265617</td>\n",
       "      <td>-2.429310</td>\n",
       "      <td>-1.073235</td>\n",
       "      <td>-90.542023</td>\n",
       "      <td>16.659401</td>\n",
       "      <td>0.454193</td>\n",
       "      <td>-17.607121</td>\n",
       "      <td>31.992260</td>\n",
       "      <td>15.967636</td>\n",
       "      <td>-51.509766</td>\n",
       "      <td>30.006121</td>\n",
       "      <td>9.053932</td>\n",
       "      <td>-75.447060</td>\n",
       "      <td>22.099865</td>\n",
       "      <td>-3.387665</td>\n",
       "      <td>-83.625824</td>\n",
       "      <td>31.320591</td>\n",
       "      <td>-11.242180</td>\n",
       "      <td>-12.971455</td>\n",
       "      <td>48.320904</td>\n",
       "      <td>0.762253</td>\n",
       "      <td>-37.152554</td>\n",
       "      <td>49.054413</td>\n",
       "      <td>-1.940414</td>\n",
       "      <td>-54.576313</td>\n",
       "      <td>42.004250</td>\n",
       "      <td>-10.960121</td>\n",
       "      <td>-65.103134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>-0.029152</td>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>50.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6.786233</td>\n",
       "      <td>-11.333852</td>\n",
       "      <td>47.534789</td>\n",
       "      <td>-20.769446</td>\n",
       "      <td>-11.812599</td>\n",
       "      <td>49.990927</td>\n",
       "      <td>-48.818829</td>\n",
       "      <td>-12.573416</td>\n",
       "      <td>14.808353</td>\n",
       "      <td>-71.526649</td>\n",
       "      <td>-9.472275</td>\n",
       "      <td>-5.685904</td>\n",
       "      <td>-86.040143</td>\n",
       "      <td>-8.497499</td>\n",
       "      <td>-20.978106</td>\n",
       "      <td>-19.892583</td>\n",
       "      <td>15.473871</td>\n",
       "      <td>-19.684725</td>\n",
       "      <td>-16.363919</td>\n",
       "      <td>31.977931</td>\n",
       "      <td>-54.562784</td>\n",
       "      <td>-20.061439</td>\n",
       "      <td>25.205292</td>\n",
       "      <td>-74.945980</td>\n",
       "      <td>-24.964155</td>\n",
       "      <td>12.585976</td>\n",
       "      <td>-82.297033</td>\n",
       "      <td>-0.656441</td>\n",
       "      <td>10.240284</td>\n",
       "      <td>-20.741277</td>\n",
       "      <td>12.169991</td>\n",
       "      <td>25.430847</td>\n",
       "      <td>-59.404511</td>\n",
       "      <td>9.774177</td>\n",
       "      <td>16.861857</td>\n",
       "      <td>-83.445555</td>\n",
       "      <td>3.000662</td>\n",
       "      <td>3.609998</td>\n",
       "      <td>-91.510464</td>\n",
       "      <td>17.612489</td>\n",
       "      <td>1.798022</td>\n",
       "      <td>-16.543760</td>\n",
       "      <td>34.630843</td>\n",
       "      <td>19.197273</td>\n",
       "      <td>-48.656146</td>\n",
       "      <td>34.708936</td>\n",
       "      <td>13.781877</td>\n",
       "      <td>-73.032392</td>\n",
       "      <td>28.021311</td>\n",
       "      <td>1.851053</td>\n",
       "      <td>-82.843496</td>\n",
       "      <td>32.317179</td>\n",
       "      <td>-9.737174</td>\n",
       "      <td>-11.650706</td>\n",
       "      <td>50.569850</td>\n",
       "      <td>3.507205</td>\n",
       "      <td>-34.192403</td>\n",
       "      <td>52.642263</td>\n",
       "      <td>1.569894</td>\n",
       "      <td>-51.594506</td>\n",
       "      <td>46.731619</td>\n",
       "      <td>-7.074512</td>\n",
       "      <td>-63.067226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>48.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5.317806</td>\n",
       "      <td>-12.499672</td>\n",
       "      <td>47.453987</td>\n",
       "      <td>-22.304150</td>\n",
       "      <td>-13.473961</td>\n",
       "      <td>48.934364</td>\n",
       "      <td>-50.567093</td>\n",
       "      <td>-11.620216</td>\n",
       "      <td>13.915695</td>\n",
       "      <td>-73.125786</td>\n",
       "      <td>-7.157860</td>\n",
       "      <td>-6.519272</td>\n",
       "      <td>-87.022034</td>\n",
       "      <td>-5.696548</td>\n",
       "      <td>-22.351597</td>\n",
       "      <td>-19.449600</td>\n",
       "      <td>15.688126</td>\n",
       "      <td>-19.963085</td>\n",
       "      <td>-15.181034</td>\n",
       "      <td>33.082428</td>\n",
       "      <td>-54.341133</td>\n",
       "      <td>-18.064960</td>\n",
       "      <td>27.075287</td>\n",
       "      <td>-75.106461</td>\n",
       "      <td>-22.477337</td>\n",
       "      <td>14.810486</td>\n",
       "      <td>-83.337776</td>\n",
       "      <td>-0.101135</td>\n",
       "      <td>10.783897</td>\n",
       "      <td>-20.485744</td>\n",
       "      <td>13.475960</td>\n",
       "      <td>26.486450</td>\n",
       "      <td>-58.701767</td>\n",
       "      <td>12.304794</td>\n",
       "      <td>18.931656</td>\n",
       "      <td>-83.193710</td>\n",
       "      <td>6.364304</td>\n",
       "      <td>6.209526</td>\n",
       "      <td>-92.698830</td>\n",
       "      <td>18.149715</td>\n",
       "      <td>2.515739</td>\n",
       "      <td>-15.874870</td>\n",
       "      <td>36.093548</td>\n",
       "      <td>20.546936</td>\n",
       "      <td>-47.148628</td>\n",
       "      <td>37.534588</td>\n",
       "      <td>16.020477</td>\n",
       "      <td>-71.687836</td>\n",
       "      <td>31.792110</td>\n",
       "      <td>4.564583</td>\n",
       "      <td>-82.641403</td>\n",
       "      <td>32.863037</td>\n",
       "      <td>-8.918983</td>\n",
       "      <td>-10.774841</td>\n",
       "      <td>52.029774</td>\n",
       "      <td>4.644127</td>\n",
       "      <td>-32.373928</td>\n",
       "      <td>55.188416</td>\n",
       "      <td>3.101257</td>\n",
       "      <td>-49.667633</td>\n",
       "      <td>50.315052</td>\n",
       "      <td>-5.215370</td>\n",
       "      <td>-61.872467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>49.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.141285</td>\n",
       "      <td>-12.493469</td>\n",
       "      <td>47.475067</td>\n",
       "      <td>-22.483795</td>\n",
       "      <td>-13.559532</td>\n",
       "      <td>48.828407</td>\n",
       "      <td>-50.828781</td>\n",
       "      <td>-11.390457</td>\n",
       "      <td>13.894257</td>\n",
       "      <td>-73.361900</td>\n",
       "      <td>-6.804001</td>\n",
       "      <td>-6.541451</td>\n",
       "      <td>-87.106232</td>\n",
       "      <td>-5.372879</td>\n",
       "      <td>-22.508564</td>\n",
       "      <td>-19.409599</td>\n",
       "      <td>15.628494</td>\n",
       "      <td>-20.048607</td>\n",
       "      <td>-15.110043</td>\n",
       "      <td>33.002945</td>\n",
       "      <td>-54.432831</td>\n",
       "      <td>-17.841034</td>\n",
       "      <td>27.203735</td>\n",
       "      <td>-75.277794</td>\n",
       "      <td>-22.137730</td>\n",
       "      <td>15.102798</td>\n",
       "      <td>-83.807457</td>\n",
       "      <td>-0.042805</td>\n",
       "      <td>10.788460</td>\n",
       "      <td>-20.483543</td>\n",
       "      <td>13.552137</td>\n",
       "      <td>26.346481</td>\n",
       "      <td>-58.752304</td>\n",
       "      <td>12.602261</td>\n",
       "      <td>18.943726</td>\n",
       "      <td>-83.300186</td>\n",
       "      <td>6.865059</td>\n",
       "      <td>6.356377</td>\n",
       "      <td>-93.104897</td>\n",
       "      <td>18.213963</td>\n",
       "      <td>2.579636</td>\n",
       "      <td>-15.790806</td>\n",
       "      <td>36.240501</td>\n",
       "      <td>20.540024</td>\n",
       "      <td>-47.057739</td>\n",
       "      <td>37.922184</td>\n",
       "      <td>16.111511</td>\n",
       "      <td>-71.599510</td>\n",
       "      <td>32.396820</td>\n",
       "      <td>4.750542</td>\n",
       "      <td>-82.761627</td>\n",
       "      <td>32.941494</td>\n",
       "      <td>-8.807564</td>\n",
       "      <td>-10.625877</td>\n",
       "      <td>52.218910</td>\n",
       "      <td>4.702339</td>\n",
       "      <td>-32.159714</td>\n",
       "      <td>55.573746</td>\n",
       "      <td>3.183128</td>\n",
       "      <td>-49.418533</td>\n",
       "      <td>50.933563</td>\n",
       "      <td>-5.084938</td>\n",
       "      <td>-61.746620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>-0.028308</td>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>43.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.931822</td>\n",
       "      <td>-12.186270</td>\n",
       "      <td>47.576642</td>\n",
       "      <td>-22.675898</td>\n",
       "      <td>-13.818677</td>\n",
       "      <td>48.666467</td>\n",
       "      <td>-51.227582</td>\n",
       "      <td>-11.307386</td>\n",
       "      <td>13.924809</td>\n",
       "      <td>-73.618568</td>\n",
       "      <td>-6.853106</td>\n",
       "      <td>-6.694705</td>\n",
       "      <td>-86.806923</td>\n",
       "      <td>-5.842213</td>\n",
       "      <td>-23.149838</td>\n",
       "      <td>-19.529686</td>\n",
       "      <td>15.138880</td>\n",
       "      <td>-20.304312</td>\n",
       "      <td>-15.416904</td>\n",
       "      <td>32.233162</td>\n",
       "      <td>-54.850683</td>\n",
       "      <td>-17.606092</td>\n",
       "      <td>27.212273</td>\n",
       "      <td>-75.955087</td>\n",
       "      <td>-21.369139</td>\n",
       "      <td>15.825144</td>\n",
       "      <td>-85.617107</td>\n",
       "      <td>-0.064653</td>\n",
       "      <td>10.699529</td>\n",
       "      <td>-20.529989</td>\n",
       "      <td>13.204302</td>\n",
       "      <td>25.702415</td>\n",
       "      <td>-59.131582</td>\n",
       "      <td>12.917689</td>\n",
       "      <td>18.736651</td>\n",
       "      <td>-83.820910</td>\n",
       "      <td>7.957404</td>\n",
       "      <td>6.580798</td>\n",
       "      <td>-94.533728</td>\n",
       "      <td>18.309981</td>\n",
       "      <td>2.890611</td>\n",
       "      <td>-15.624303</td>\n",
       "      <td>36.167134</td>\n",
       "      <td>20.690137</td>\n",
       "      <td>-47.079395</td>\n",
       "      <td>38.460759</td>\n",
       "      <td>16.520139</td>\n",
       "      <td>-71.615142</td>\n",
       "      <td>33.674308</td>\n",
       "      <td>5.398041</td>\n",
       "      <td>-83.336438</td>\n",
       "      <td>33.217521</td>\n",
       "      <td>-8.166978</td>\n",
       "      <td>-10.266741</td>\n",
       "      <td>52.535532</td>\n",
       "      <td>5.324988</td>\n",
       "      <td>-31.775413</td>\n",
       "      <td>56.404622</td>\n",
       "      <td>3.893927</td>\n",
       "      <td>-48.932433</td>\n",
       "      <td>52.530867</td>\n",
       "      <td>-4.270894</td>\n",
       "      <td>-61.584274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Leap timestamp  timestamp diff  emg timestamp   ch1    ch2    ch3   ch4  \\\n",
       "0    1.571891e+09        0.006226   1.571891e+09  50.0  250.0  396.0  89.0   \n",
       "1    1.571891e+09       -0.029152   1.571891e+09  50.0  286.0  368.0  74.0   \n",
       "2    1.571891e+09        0.001225   1.571891e+09  48.0  270.0  347.0  69.0   \n",
       "3    1.571891e+09        0.009088   1.571891e+09  49.0  245.0  326.0  66.0   \n",
       "4    1.571891e+09       -0.028308   1.571891e+09  43.0  247.0  325.0  49.0   \n",
       "\n",
       "    ch5   ch6   ch7   ch8   Wrist x    Wrist y    Wrist z  Thumb Proximal x  \\\n",
       "0  53.0  43.0  31.0  61.0  9.262085  -9.118690  47.617905        -18.114399   \n",
       "1  51.0  29.0  23.0  42.0  6.786233 -11.333852  47.534789        -20.769446   \n",
       "2  50.0  29.0  22.0  37.0  5.317806 -12.499672  47.453987        -22.304150   \n",
       "3  39.0  28.0  23.0  36.0  5.141285 -12.493469  47.475067        -22.483795   \n",
       "4  33.0  25.0  20.0  34.0  4.931822 -12.186270  47.576642        -22.675898   \n",
       "\n",
       "   Thumb Proximal y  Thumb Proximal z  Thumb Intermediate x  \\\n",
       "0         -8.678528         51.673462            -46.709621   \n",
       "1        -11.812599         49.990927            -48.818829   \n",
       "2        -13.473961         48.934364            -50.567093   \n",
       "3        -13.559532         48.828407            -50.828781   \n",
       "4        -13.818677         48.666467            -51.227582   \n",
       "\n",
       "   Thumb Intermediate y  Thumb Intermediate z  Thumb Distal x  Thumb Distal y  \\\n",
       "0            -12.567474             17.094170      -69.877258      -11.163055   \n",
       "1            -12.573416             14.808353      -71.526649       -9.472275   \n",
       "2            -11.620216             13.915695      -73.125786       -7.157860   \n",
       "3            -11.390457             13.894257      -73.361900       -6.804001   \n",
       "4            -11.307386             13.924809      -73.618568       -6.853106   \n",
       "\n",
       "   Thumb Distal z  Thumb Tip x  Thumb Tip y  Thumb Tip z  Index Proximal x  \\\n",
       "0       -3.096992   -85.138763   -10.990265   -17.690151        -20.603283   \n",
       "1       -5.685904   -86.040143    -8.497499   -20.978106        -19.892583   \n",
       "2       -6.519272   -87.022034    -5.696548   -22.351597        -19.449600   \n",
       "3       -6.541451   -87.106232    -5.372879   -22.508564        -19.409599   \n",
       "4       -6.694705   -86.806923    -5.842213   -23.149838        -19.529686   \n",
       "\n",
       "   Index Proximal y  Index Proximal z  Index Intermediate x  \\\n",
       "0         15.033867        -19.298096            -18.594982   \n",
       "1         15.473871        -19.684725            -16.363919   \n",
       "2         15.688126        -19.963085            -15.181034   \n",
       "3         15.628494        -20.048607            -15.110043   \n",
       "4         15.138880        -20.304312            -15.416904   \n",
       "\n",
       "   Index Intermediate y  Index Intermediate z  Index Distal x  Index Distal y  \\\n",
       "0             29.675285            -55.134228      -23.628544       21.952751   \n",
       "1             31.977931            -54.562784      -20.061439       25.205292   \n",
       "2             33.082428            -54.341133      -18.064960       27.075287   \n",
       "3             33.002945            -54.432831      -17.841034       27.203735   \n",
       "4             32.233162            -54.850683      -17.606092       27.212273   \n",
       "\n",
       "   Index Distal z  Index Tip x  Index Tip y  Index Tip z  Middle Proximal x  \\\n",
       "0      -74.898529   -29.280987     9.089638   -81.241966          -1.600807   \n",
       "1      -74.945980   -24.964155    12.585976   -82.297033          -0.656441   \n",
       "2      -75.106461   -22.477337    14.810486   -83.337776          -0.101135   \n",
       "3      -75.277794   -22.137730    15.102798   -83.807457          -0.042805   \n",
       "4      -75.955087   -21.369139    15.825144   -85.617107          -0.064653   \n",
       "\n",
       "   Middle Proximal y  Middle Proximal z  Middle Intermediate x  \\\n",
       "0           9.199806         -21.184177               9.602978   \n",
       "1          10.240284         -20.741277              12.169991   \n",
       "2          10.783897         -20.485744              13.475960   \n",
       "3          10.788460         -20.483543              13.552137   \n",
       "4          10.699529         -20.529989              13.204302   \n",
       "\n",
       "   Middle Intermediate y  Middle Intermediate z  Middle Distal x  \\\n",
       "0              22.597313             -61.013245         5.366776   \n",
       "1              25.430847             -59.404511         9.774177   \n",
       "2              26.486450             -58.701767        12.304794   \n",
       "3              26.346481             -58.752304        12.602261   \n",
       "4              25.702415             -59.131582        12.917689   \n",
       "\n",
       "   Middle Distal y  Middle Distal z  Middle Tip x  Middle Tip y  Middle Tip z  \\\n",
       "0        12.613152       -84.265617     -2.429310     -1.073235    -90.542023   \n",
       "1        16.861857       -83.445555      3.000662      3.609998    -91.510464   \n",
       "2        18.931656       -83.193710      6.364304      6.209526    -92.698830   \n",
       "3        18.943726       -83.300186      6.865059      6.356377    -93.104897   \n",
       "4        18.736651       -83.820910      7.957404      6.580798    -94.533728   \n",
       "\n",
       "   Ring Proximal x  Ring Proximal y  Ring Proximal z  Ring Intermediate x  \\\n",
       "0        16.659401         0.454193       -17.607121            31.992260   \n",
       "1        17.612489         1.798022       -16.543760            34.630843   \n",
       "2        18.149715         2.515739       -15.874870            36.093548   \n",
       "3        18.213963         2.579636       -15.790806            36.240501   \n",
       "4        18.309981         2.890611       -15.624303            36.167134   \n",
       "\n",
       "   Ring Intermediate y  Ring Intermediate z  Ring Distal x  Ring Distal y  \\\n",
       "0            15.967636           -51.509766      30.006121       9.053932   \n",
       "1            19.197273           -48.656146      34.708936      13.781877   \n",
       "2            20.546936           -47.148628      37.534588      16.020477   \n",
       "3            20.540024           -47.057739      37.922184      16.111511   \n",
       "4            20.690137           -47.079395      38.460759      16.520139   \n",
       "\n",
       "   Ring Distal z  Ring Tip x  Ring Tip y  Ring Tip z  Pinky Proximal x  \\\n",
       "0     -75.447060   22.099865   -3.387665  -83.625824         31.320591   \n",
       "1     -73.032392   28.021311    1.851053  -82.843496         32.317179   \n",
       "2     -71.687836   31.792110    4.564583  -82.641403         32.863037   \n",
       "3     -71.599510   32.396820    4.750542  -82.761627         32.941494   \n",
       "4     -71.615142   33.674308    5.398041  -83.336438         33.217521   \n",
       "\n",
       "   Pinky Proximal y  Pinky Proximal z  Pinky Intermediate x  \\\n",
       "0        -11.242180        -12.971455             48.320904   \n",
       "1         -9.737174        -11.650706             50.569850   \n",
       "2         -8.918983        -10.774841             52.029774   \n",
       "3         -8.807564        -10.625877             52.218910   \n",
       "4         -8.166978        -10.266741             52.535532   \n",
       "\n",
       "   Pinky Intermediate y  Pinky Intermediate z  Pinky Distal x  Pinky Distal y  \\\n",
       "0              0.762253            -37.152554       49.054413       -1.940414   \n",
       "1              3.507205            -34.192403       52.642263        1.569894   \n",
       "2              4.644127            -32.373928       55.188416        3.101257   \n",
       "3              4.702339            -32.159714       55.573746        3.183128   \n",
       "4              5.324988            -31.775413       56.404622        3.893927   \n",
       "\n",
       "   Pinky Distal z  Pinky Tip x  Pinky Tip y  Pinky Tip z  \n",
       "0      -54.576313    42.004250   -10.960121   -65.103134  \n",
       "1      -51.594506    46.731619    -7.074512   -63.067226  \n",
       "2      -49.667633    50.315052    -5.215370   -61.872467  \n",
       "3      -49.418533    50.933563    -5.084938   -61.746620  \n",
       "4      -48.932433    52.530867    -4.270894   -61.584274  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels=[\"Leap timestamp\", \"timestamp diff\", \"emg timestamp\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ch1</th>\n",
       "      <th>ch2</th>\n",
       "      <th>ch3</th>\n",
       "      <th>ch4</th>\n",
       "      <th>ch5</th>\n",
       "      <th>ch6</th>\n",
       "      <th>ch7</th>\n",
       "      <th>ch8</th>\n",
       "      <th>Wrist x</th>\n",
       "      <th>Wrist y</th>\n",
       "      <th>Wrist z</th>\n",
       "      <th>Thumb Proximal x</th>\n",
       "      <th>Thumb Proximal y</th>\n",
       "      <th>Thumb Proximal z</th>\n",
       "      <th>Thumb Intermediate x</th>\n",
       "      <th>Thumb Intermediate y</th>\n",
       "      <th>Thumb Intermediate z</th>\n",
       "      <th>Thumb Distal x</th>\n",
       "      <th>Thumb Distal y</th>\n",
       "      <th>Thumb Distal z</th>\n",
       "      <th>Thumb Tip x</th>\n",
       "      <th>Thumb Tip y</th>\n",
       "      <th>Thumb Tip z</th>\n",
       "      <th>Index Proximal x</th>\n",
       "      <th>Index Proximal y</th>\n",
       "      <th>Index Proximal z</th>\n",
       "      <th>Index Intermediate x</th>\n",
       "      <th>Index Intermediate y</th>\n",
       "      <th>Index Intermediate z</th>\n",
       "      <th>Index Distal x</th>\n",
       "      <th>Index Distal y</th>\n",
       "      <th>Index Distal z</th>\n",
       "      <th>Index Tip x</th>\n",
       "      <th>Index Tip y</th>\n",
       "      <th>Index Tip z</th>\n",
       "      <th>Middle Proximal x</th>\n",
       "      <th>Middle Proximal y</th>\n",
       "      <th>Middle Proximal z</th>\n",
       "      <th>Middle Intermediate x</th>\n",
       "      <th>Middle Intermediate y</th>\n",
       "      <th>Middle Intermediate z</th>\n",
       "      <th>Middle Distal x</th>\n",
       "      <th>Middle Distal y</th>\n",
       "      <th>Middle Distal z</th>\n",
       "      <th>Middle Tip x</th>\n",
       "      <th>Middle Tip y</th>\n",
       "      <th>Middle Tip z</th>\n",
       "      <th>Ring Proximal x</th>\n",
       "      <th>Ring Proximal y</th>\n",
       "      <th>Ring Proximal z</th>\n",
       "      <th>Ring Intermediate x</th>\n",
       "      <th>Ring Intermediate y</th>\n",
       "      <th>Ring Intermediate z</th>\n",
       "      <th>Ring Distal x</th>\n",
       "      <th>Ring Distal y</th>\n",
       "      <th>Ring Distal z</th>\n",
       "      <th>Ring Tip x</th>\n",
       "      <th>Ring Tip y</th>\n",
       "      <th>Ring Tip z</th>\n",
       "      <th>Pinky Proximal x</th>\n",
       "      <th>Pinky Proximal y</th>\n",
       "      <th>Pinky Proximal z</th>\n",
       "      <th>Pinky Intermediate x</th>\n",
       "      <th>Pinky Intermediate y</th>\n",
       "      <th>Pinky Intermediate z</th>\n",
       "      <th>Pinky Distal x</th>\n",
       "      <th>Pinky Distal y</th>\n",
       "      <th>Pinky Distal z</th>\n",
       "      <th>Pinky Tip x</th>\n",
       "      <th>Pinky Tip y</th>\n",
       "      <th>Pinky Tip z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>107.959076</td>\n",
       "      <td>277.776554</td>\n",
       "      <td>293.447060</td>\n",
       "      <td>112.697202</td>\n",
       "      <td>55.729031</td>\n",
       "      <td>72.685406</td>\n",
       "      <td>82.720188</td>\n",
       "      <td>67.966208</td>\n",
       "      <td>-1.519532</td>\n",
       "      <td>-9.021946</td>\n",
       "      <td>44.597009</td>\n",
       "      <td>-24.352507</td>\n",
       "      <td>-18.957473</td>\n",
       "      <td>39.809937</td>\n",
       "      <td>-35.202575</td>\n",
       "      <td>-26.591242</td>\n",
       "      <td>2.103636</td>\n",
       "      <td>-40.789788</td>\n",
       "      <td>-32.012339</td>\n",
       "      <td>-22.847791</td>\n",
       "      <td>-44.154327</td>\n",
       "      <td>-35.382372</td>\n",
       "      <td>-37.740631</td>\n",
       "      <td>-16.995893</td>\n",
       "      <td>7.373415</td>\n",
       "      <td>-23.683015</td>\n",
       "      <td>-14.404202</td>\n",
       "      <td>3.981787</td>\n",
       "      <td>-54.751712</td>\n",
       "      <td>-13.055133</td>\n",
       "      <td>-3.578775</td>\n",
       "      <td>-67.611467</td>\n",
       "      <td>-12.313513</td>\n",
       "      <td>-10.208400</td>\n",
       "      <td>-73.972310</td>\n",
       "      <td>0.712549</td>\n",
       "      <td>9.388218</td>\n",
       "      <td>-19.306369</td>\n",
       "      <td>9.581468</td>\n",
       "      <td>4.679483</td>\n",
       "      <td>-53.065222</td>\n",
       "      <td>12.692946</td>\n",
       "      <td>-4.684326</td>\n",
       "      <td>-67.946889</td>\n",
       "      <td>13.756485</td>\n",
       "      <td>-12.317275</td>\n",
       "      <td>-74.908446</td>\n",
       "      <td>17.529791</td>\n",
       "      <td>8.095233</td>\n",
       "      <td>-10.276684</td>\n",
       "      <td>25.648988</td>\n",
       "      <td>-0.778659</td>\n",
       "      <td>-40.999281</td>\n",
       "      <td>27.275005</td>\n",
       "      <td>-12.593958</td>\n",
       "      <td>-54.669632</td>\n",
       "      <td>27.006531</td>\n",
       "      <td>-21.898478</td>\n",
       "      <td>-60.856092</td>\n",
       "      <td>32.352225</td>\n",
       "      <td>2.837640</td>\n",
       "      <td>-1.285509</td>\n",
       "      <td>41.970822</td>\n",
       "      <td>-4.485664</td>\n",
       "      <td>-24.370822</td>\n",
       "      <td>43.496761</td>\n",
       "      <td>-12.840841</td>\n",
       "      <td>-34.176921</td>\n",
       "      <td>42.845118</td>\n",
       "      <td>-21.385414</td>\n",
       "      <td>-40.369203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>94.478903</td>\n",
       "      <td>183.819378</td>\n",
       "      <td>181.553457</td>\n",
       "      <td>80.348375</td>\n",
       "      <td>68.751875</td>\n",
       "      <td>110.614351</td>\n",
       "      <td>137.870160</td>\n",
       "      <td>103.867191</td>\n",
       "      <td>18.203014</td>\n",
       "      <td>13.902148</td>\n",
       "      <td>5.902738</td>\n",
       "      <td>17.833762</td>\n",
       "      <td>17.313549</td>\n",
       "      <td>12.854626</td>\n",
       "      <td>10.539880</td>\n",
       "      <td>13.674356</td>\n",
       "      <td>17.937670</td>\n",
       "      <td>20.180079</td>\n",
       "      <td>18.120903</td>\n",
       "      <td>22.667319</td>\n",
       "      <td>30.208039</td>\n",
       "      <td>23.253477</td>\n",
       "      <td>26.870641</td>\n",
       "      <td>9.447936</td>\n",
       "      <td>7.232516</td>\n",
       "      <td>7.711189</td>\n",
       "      <td>22.331166</td>\n",
       "      <td>23.019437</td>\n",
       "      <td>11.387768</td>\n",
       "      <td>28.566881</td>\n",
       "      <td>33.673159</td>\n",
       "      <td>18.030049</td>\n",
       "      <td>32.359781</td>\n",
       "      <td>40.269171</td>\n",
       "      <td>25.337680</td>\n",
       "      <td>8.255159</td>\n",
       "      <td>6.050276</td>\n",
       "      <td>3.587558</td>\n",
       "      <td>22.631948</td>\n",
       "      <td>25.144259</td>\n",
       "      <td>10.117039</td>\n",
       "      <td>30.242326</td>\n",
       "      <td>37.583324</td>\n",
       "      <td>18.901129</td>\n",
       "      <td>34.760672</td>\n",
       "      <td>44.641463</td>\n",
       "      <td>27.063839</td>\n",
       "      <td>5.532313</td>\n",
       "      <td>7.853489</td>\n",
       "      <td>8.013560</td>\n",
       "      <td>18.961775</td>\n",
       "      <td>22.917322</td>\n",
       "      <td>13.537246</td>\n",
       "      <td>27.110144</td>\n",
       "      <td>33.080043</td>\n",
       "      <td>21.107323</td>\n",
       "      <td>32.277698</td>\n",
       "      <td>38.685426</td>\n",
       "      <td>28.408468</td>\n",
       "      <td>4.083498</td>\n",
       "      <td>11.077100</td>\n",
       "      <td>13.504016</td>\n",
       "      <td>14.765910</td>\n",
       "      <td>20.311902</td>\n",
       "      <td>18.142483</td>\n",
       "      <td>21.859076</td>\n",
       "      <td>26.318486</td>\n",
       "      <td>21.132027</td>\n",
       "      <td>28.005897</td>\n",
       "      <td>30.720860</td>\n",
       "      <td>25.151496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>-49.460617</td>\n",
       "      <td>-51.661484</td>\n",
       "      <td>-8.476379</td>\n",
       "      <td>-59.220070</td>\n",
       "      <td>-58.296936</td>\n",
       "      <td>-27.315063</td>\n",
       "      <td>-70.195053</td>\n",
       "      <td>-71.954811</td>\n",
       "      <td>-49.751572</td>\n",
       "      <td>-92.561234</td>\n",
       "      <td>-90.297638</td>\n",
       "      <td>-74.886850</td>\n",
       "      <td>-112.754237</td>\n",
       "      <td>-103.504150</td>\n",
       "      <td>-94.854360</td>\n",
       "      <td>-35.338264</td>\n",
       "      <td>-31.792175</td>\n",
       "      <td>-35.444645</td>\n",
       "      <td>-69.852450</td>\n",
       "      <td>-67.396919</td>\n",
       "      <td>-74.289215</td>\n",
       "      <td>-85.011714</td>\n",
       "      <td>-87.236955</td>\n",
       "      <td>-96.546054</td>\n",
       "      <td>-97.884471</td>\n",
       "      <td>-102.978952</td>\n",
       "      <td>-112.462585</td>\n",
       "      <td>-24.145329</td>\n",
       "      <td>-21.980112</td>\n",
       "      <td>-25.657837</td>\n",
       "      <td>-54.249256</td>\n",
       "      <td>-65.532608</td>\n",
       "      <td>-72.724304</td>\n",
       "      <td>-75.109543</td>\n",
       "      <td>-91.424866</td>\n",
       "      <td>-100.599316</td>\n",
       "      <td>-91.211243</td>\n",
       "      <td>-109.174942</td>\n",
       "      <td>-118.985252</td>\n",
       "      <td>-12.639398</td>\n",
       "      <td>-25.039429</td>\n",
       "      <td>-26.869192</td>\n",
       "      <td>-40.555109</td>\n",
       "      <td>-60.490051</td>\n",
       "      <td>-66.913315</td>\n",
       "      <td>-59.266098</td>\n",
       "      <td>-81.102333</td>\n",
       "      <td>-93.231049</td>\n",
       "      <td>-75.195114</td>\n",
       "      <td>-92.351674</td>\n",
       "      <td>-111.118563</td>\n",
       "      <td>-30.062690</td>\n",
       "      <td>-36.622925</td>\n",
       "      <td>-36.324131</td>\n",
       "      <td>-37.905338</td>\n",
       "      <td>-56.838379</td>\n",
       "      <td>-66.809570</td>\n",
       "      <td>-43.796612</td>\n",
       "      <td>-71.155746</td>\n",
       "      <td>-80.829399</td>\n",
       "      <td>-53.352108</td>\n",
       "      <td>-81.252701</td>\n",
       "      <td>-96.391819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>-16.886156</td>\n",
       "      <td>-17.755783</td>\n",
       "      <td>42.359551</td>\n",
       "      <td>-39.496848</td>\n",
       "      <td>-31.326163</td>\n",
       "      <td>31.505045</td>\n",
       "      <td>-42.614314</td>\n",
       "      <td>-35.798828</td>\n",
       "      <td>-11.512680</td>\n",
       "      <td>-56.313196</td>\n",
       "      <td>-44.501390</td>\n",
       "      <td>-40.588364</td>\n",
       "      <td>-68.302795</td>\n",
       "      <td>-52.288237</td>\n",
       "      <td>-58.127181</td>\n",
       "      <td>-26.110477</td>\n",
       "      <td>2.451675</td>\n",
       "      <td>-30.127332</td>\n",
       "      <td>-35.800728</td>\n",
       "      <td>-14.385666</td>\n",
       "      <td>-63.962860</td>\n",
       "      <td>-39.267851</td>\n",
       "      <td>-32.875201</td>\n",
       "      <td>-81.408615</td>\n",
       "      <td>-41.107635</td>\n",
       "      <td>-45.302944</td>\n",
       "      <td>-93.672626</td>\n",
       "      <td>-6.837919</td>\n",
       "      <td>5.863414</td>\n",
       "      <td>-21.728305</td>\n",
       "      <td>-11.149257</td>\n",
       "      <td>-16.099387</td>\n",
       "      <td>-60.206032</td>\n",
       "      <td>-13.197766</td>\n",
       "      <td>-38.103338</td>\n",
       "      <td>-81.200862</td>\n",
       "      <td>-14.475380</td>\n",
       "      <td>-51.520982</td>\n",
       "      <td>-94.599333</td>\n",
       "      <td>13.344295</td>\n",
       "      <td>2.828438</td>\n",
       "      <td>-17.730928</td>\n",
       "      <td>10.075094</td>\n",
       "      <td>-19.526822</td>\n",
       "      <td>-51.560223</td>\n",
       "      <td>6.909097</td>\n",
       "      <td>-40.784522</td>\n",
       "      <td>-69.958648</td>\n",
       "      <td>3.556110</td>\n",
       "      <td>-54.368904</td>\n",
       "      <td>-81.196796</td>\n",
       "      <td>30.378026</td>\n",
       "      <td>-4.798233</td>\n",
       "      <td>-14.394444</td>\n",
       "      <td>32.979874</td>\n",
       "      <td>-20.168663</td>\n",
       "      <td>-40.403976</td>\n",
       "      <td>28.580751</td>\n",
       "      <td>-33.926491</td>\n",
       "      <td>-51.121618</td>\n",
       "      <td>22.435043</td>\n",
       "      <td>-45.941994</td>\n",
       "      <td>-58.652584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>-6.817474</td>\n",
       "      <td>-8.803251</td>\n",
       "      <td>45.916725</td>\n",
       "      <td>-28.895584</td>\n",
       "      <td>-20.031019</td>\n",
       "      <td>40.402634</td>\n",
       "      <td>-35.806393</td>\n",
       "      <td>-27.875320</td>\n",
       "      <td>-0.243591</td>\n",
       "      <td>-42.077468</td>\n",
       "      <td>-34.920914</td>\n",
       "      <td>-27.283134</td>\n",
       "      <td>-46.161587</td>\n",
       "      <td>-39.915314</td>\n",
       "      <td>-43.178146</td>\n",
       "      <td>-15.925140</td>\n",
       "      <td>6.985863</td>\n",
       "      <td>-26.454096</td>\n",
       "      <td>-9.026079</td>\n",
       "      <td>4.463394</td>\n",
       "      <td>-56.602979</td>\n",
       "      <td>-6.701809</td>\n",
       "      <td>-5.036469</td>\n",
       "      <td>-71.388425</td>\n",
       "      <td>-6.119522</td>\n",
       "      <td>-14.418533</td>\n",
       "      <td>-80.410423</td>\n",
       "      <td>2.151676</td>\n",
       "      <td>9.575974</td>\n",
       "      <td>-20.124603</td>\n",
       "      <td>15.232921</td>\n",
       "      <td>4.461868</td>\n",
       "      <td>-54.737091</td>\n",
       "      <td>17.435110</td>\n",
       "      <td>-7.066628</td>\n",
       "      <td>-72.400841</td>\n",
       "      <td>15.876219</td>\n",
       "      <td>-17.552639</td>\n",
       "      <td>-82.020462</td>\n",
       "      <td>18.373080</td>\n",
       "      <td>8.872391</td>\n",
       "      <td>-9.174782</td>\n",
       "      <td>29.783724</td>\n",
       "      <td>-2.790802</td>\n",
       "      <td>-41.144341</td>\n",
       "      <td>28.554492</td>\n",
       "      <td>-17.606827</td>\n",
       "      <td>-56.429403</td>\n",
       "      <td>24.772669</td>\n",
       "      <td>-29.923187</td>\n",
       "      <td>-64.962646</td>\n",
       "      <td>33.172597</td>\n",
       "      <td>3.786240</td>\n",
       "      <td>2.370359</td>\n",
       "      <td>43.076133</td>\n",
       "      <td>-7.020432</td>\n",
       "      <td>-22.701141</td>\n",
       "      <td>43.237972</td>\n",
       "      <td>-18.004105</td>\n",
       "      <td>-32.667877</td>\n",
       "      <td>41.146488</td>\n",
       "      <td>-28.819870</td>\n",
       "      <td>-38.904312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>124.000000</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>16.217522</td>\n",
       "      <td>-0.516635</td>\n",
       "      <td>48.589417</td>\n",
       "      <td>-8.741714</td>\n",
       "      <td>-7.933365</td>\n",
       "      <td>50.493407</td>\n",
       "      <td>-28.042894</td>\n",
       "      <td>-19.092545</td>\n",
       "      <td>16.548199</td>\n",
       "      <td>-26.790266</td>\n",
       "      <td>-22.851822</td>\n",
       "      <td>-7.325100</td>\n",
       "      <td>-23.168304</td>\n",
       "      <td>-23.262767</td>\n",
       "      <td>-21.588488</td>\n",
       "      <td>-9.653668</td>\n",
       "      <td>12.051170</td>\n",
       "      <td>-17.442369</td>\n",
       "      <td>3.830158</td>\n",
       "      <td>21.979660</td>\n",
       "      <td>-48.346376</td>\n",
       "      <td>9.978733</td>\n",
       "      <td>24.674629</td>\n",
       "      <td>-58.375332</td>\n",
       "      <td>13.583131</td>\n",
       "      <td>23.936172</td>\n",
       "      <td>-60.066422</td>\n",
       "      <td>7.391723</td>\n",
       "      <td>13.382727</td>\n",
       "      <td>-17.935727</td>\n",
       "      <td>27.901146</td>\n",
       "      <td>25.355267</td>\n",
       "      <td>-48.052736</td>\n",
       "      <td>37.783436</td>\n",
       "      <td>27.261452</td>\n",
       "      <td>-59.167010</td>\n",
       "      <td>43.332731</td>\n",
       "      <td>25.106003</td>\n",
       "      <td>-60.922623</td>\n",
       "      <td>22.278203</td>\n",
       "      <td>14.082314</td>\n",
       "      <td>-4.153049</td>\n",
       "      <td>41.117867</td>\n",
       "      <td>16.907257</td>\n",
       "      <td>-32.404678</td>\n",
       "      <td>50.301309</td>\n",
       "      <td>12.930918</td>\n",
       "      <td>-43.088371</td>\n",
       "      <td>54.606854</td>\n",
       "      <td>6.906043</td>\n",
       "      <td>-45.007335</td>\n",
       "      <td>35.187468</td>\n",
       "      <td>10.987781</td>\n",
       "      <td>9.982524</td>\n",
       "      <td>54.273039</td>\n",
       "      <td>11.055683</td>\n",
       "      <td>-9.839353</td>\n",
       "      <td>62.777967</td>\n",
       "      <td>6.611061</td>\n",
       "      <td>-18.714948</td>\n",
       "      <td>67.228989</td>\n",
       "      <td>-0.268571</td>\n",
       "      <td>-23.747971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1597.000000</td>\n",
       "      <td>1524.000000</td>\n",
       "      <td>1622.000000</td>\n",
       "      <td>1841.000000</td>\n",
       "      <td>1822.000000</td>\n",
       "      <td>1776.000000</td>\n",
       "      <td>1665.000000</td>\n",
       "      <td>1869.000000</td>\n",
       "      <td>52.359741</td>\n",
       "      <td>50.699184</td>\n",
       "      <td>54.694229</td>\n",
       "      <td>48.278114</td>\n",
       "      <td>55.201233</td>\n",
       "      <td>61.440742</td>\n",
       "      <td>14.705590</td>\n",
       "      <td>48.795957</td>\n",
       "      <td>66.071671</td>\n",
       "      <td>33.443270</td>\n",
       "      <td>72.289636</td>\n",
       "      <td>81.401413</td>\n",
       "      <td>50.398455</td>\n",
       "      <td>93.400940</td>\n",
       "      <td>96.671875</td>\n",
       "      <td>29.582220</td>\n",
       "      <td>33.404694</td>\n",
       "      <td>15.407104</td>\n",
       "      <td>55.390396</td>\n",
       "      <td>62.662933</td>\n",
       "      <td>33.897133</td>\n",
       "      <td>73.737137</td>\n",
       "      <td>82.416367</td>\n",
       "      <td>46.695574</td>\n",
       "      <td>87.999207</td>\n",
       "      <td>96.914932</td>\n",
       "      <td>53.381760</td>\n",
       "      <td>23.445007</td>\n",
       "      <td>25.185989</td>\n",
       "      <td>6.137421</td>\n",
       "      <td>66.375984</td>\n",
       "      <td>69.576004</td>\n",
       "      <td>25.039421</td>\n",
       "      <td>88.228264</td>\n",
       "      <td>94.551773</td>\n",
       "      <td>44.773242</td>\n",
       "      <td>102.207489</td>\n",
       "      <td>111.433588</td>\n",
       "      <td>55.553363</td>\n",
       "      <td>26.836068</td>\n",
       "      <td>25.465968</td>\n",
       "      <td>18.247375</td>\n",
       "      <td>65.926038</td>\n",
       "      <td>64.414276</td>\n",
       "      <td>41.517410</td>\n",
       "      <td>90.692032</td>\n",
       "      <td>86.780737</td>\n",
       "      <td>58.788498</td>\n",
       "      <td>107.192108</td>\n",
       "      <td>102.598709</td>\n",
       "      <td>70.328949</td>\n",
       "      <td>39.572960</td>\n",
       "      <td>34.939362</td>\n",
       "      <td>35.891083</td>\n",
       "      <td>69.175903</td>\n",
       "      <td>53.101288</td>\n",
       "      <td>52.591827</td>\n",
       "      <td>85.342997</td>\n",
       "      <td>67.743378</td>\n",
       "      <td>65.750656</td>\n",
       "      <td>101.063904</td>\n",
       "      <td>81.727936</td>\n",
       "      <td>76.889313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ch1            ch2            ch3            ch4  \\\n",
       "count  106979.000000  106979.000000  106979.000000  106979.000000   \n",
       "mean      107.959076     277.776554     293.447060     112.697202   \n",
       "std        94.478903     183.819378     181.553457      80.348375   \n",
       "min        18.000000      20.000000      22.000000      20.000000   \n",
       "25%        61.000000     143.000000     152.000000      72.000000   \n",
       "50%        87.000000     232.000000     260.000000      94.000000   \n",
       "75%       124.000000     379.000000     398.000000     127.000000   \n",
       "max      1597.000000    1524.000000    1622.000000    1841.000000   \n",
       "\n",
       "                 ch5            ch6            ch7            ch8  \\\n",
       "count  106979.000000  106979.000000  106979.000000  106979.000000   \n",
       "mean       55.729031      72.685406      82.720188      67.966208   \n",
       "std        68.751875     110.614351     137.870160     103.867191   \n",
       "min        17.000000      16.000000      15.000000      15.000000   \n",
       "25%        33.000000      28.000000      26.000000      27.000000   \n",
       "50%        40.000000      33.000000      31.000000      34.000000   \n",
       "75%        58.000000      59.000000      66.000000      56.000000   \n",
       "max      1822.000000    1776.000000    1665.000000    1869.000000   \n",
       "\n",
       "             Wrist x        Wrist y        Wrist z  Thumb Proximal x  \\\n",
       "count  106979.000000  106979.000000  106979.000000     106979.000000   \n",
       "mean       -1.519532      -9.021946      44.597009        -24.352507   \n",
       "std        18.203014      13.902148       5.902738         17.833762   \n",
       "min       -49.460617     -51.661484      -8.476379        -59.220070   \n",
       "25%       -16.886156     -17.755783      42.359551        -39.496848   \n",
       "50%        -6.817474      -8.803251      45.916725        -28.895584   \n",
       "75%        16.217522      -0.516635      48.589417         -8.741714   \n",
       "max        52.359741      50.699184      54.694229         48.278114   \n",
       "\n",
       "       Thumb Proximal y  Thumb Proximal z  Thumb Intermediate x  \\\n",
       "count     106979.000000     106979.000000         106979.000000   \n",
       "mean         -18.957473         39.809937            -35.202575   \n",
       "std           17.313549         12.854626             10.539880   \n",
       "min          -58.296936        -27.315063            -70.195053   \n",
       "25%          -31.326163         31.505045            -42.614314   \n",
       "50%          -20.031019         40.402634            -35.806393   \n",
       "75%           -7.933365         50.493407            -28.042894   \n",
       "max           55.201233         61.440742             14.705590   \n",
       "\n",
       "       Thumb Intermediate y  Thumb Intermediate z  Thumb Distal x  \\\n",
       "count         106979.000000         106979.000000   106979.000000   \n",
       "mean             -26.591242              2.103636      -40.789788   \n",
       "std               13.674356             17.937670       20.180079   \n",
       "min              -71.954811            -49.751572      -92.561234   \n",
       "25%              -35.798828            -11.512680      -56.313196   \n",
       "50%              -27.875320             -0.243591      -42.077468   \n",
       "75%              -19.092545             16.548199      -26.790266   \n",
       "max               48.795957             66.071671       33.443270   \n",
       "\n",
       "       Thumb Distal y  Thumb Distal z    Thumb Tip x    Thumb Tip y  \\\n",
       "count   106979.000000   106979.000000  106979.000000  106979.000000   \n",
       "mean       -32.012339      -22.847791     -44.154327     -35.382372   \n",
       "std         18.120903       22.667319      30.208039      23.253477   \n",
       "min        -90.297638      -74.886850    -112.754237    -103.504150   \n",
       "25%        -44.501390      -40.588364     -68.302795     -52.288237   \n",
       "50%        -34.920914      -27.283134     -46.161587     -39.915314   \n",
       "75%        -22.851822       -7.325100     -23.168304     -23.262767   \n",
       "max         72.289636       81.401413      50.398455      93.400940   \n",
       "\n",
       "         Thumb Tip z  Index Proximal x  Index Proximal y  Index Proximal z  \\\n",
       "count  106979.000000     106979.000000     106979.000000     106979.000000   \n",
       "mean      -37.740631        -16.995893          7.373415        -23.683015   \n",
       "std        26.870641          9.447936          7.232516          7.711189   \n",
       "min       -94.854360        -35.338264        -31.792175        -35.444645   \n",
       "25%       -58.127181        -26.110477          2.451675        -30.127332   \n",
       "50%       -43.178146        -15.925140          6.985863        -26.454096   \n",
       "75%       -21.588488         -9.653668         12.051170        -17.442369   \n",
       "max        96.671875         29.582220         33.404694         15.407104   \n",
       "\n",
       "       Index Intermediate x  Index Intermediate y  Index Intermediate z  \\\n",
       "count         106979.000000         106979.000000         106979.000000   \n",
       "mean             -14.404202              3.981787            -54.751712   \n",
       "std               22.331166             23.019437             11.387768   \n",
       "min              -69.852450            -67.396919            -74.289215   \n",
       "25%              -35.800728            -14.385666            -63.962860   \n",
       "50%               -9.026079              4.463394            -56.602979   \n",
       "75%                3.830158             21.979660            -48.346376   \n",
       "max               55.390396             62.662933             33.897133   \n",
       "\n",
       "       Index Distal x  Index Distal y  Index Distal z    Index Tip x  \\\n",
       "count   106979.000000   106979.000000   106979.000000  106979.000000   \n",
       "mean       -13.055133       -3.578775      -67.611467     -12.313513   \n",
       "std         28.566881       33.673159       18.030049      32.359781   \n",
       "min        -85.011714      -87.236955      -96.546054     -97.884471   \n",
       "25%        -39.267851      -32.875201      -81.408615     -41.107635   \n",
       "50%         -6.701809       -5.036469      -71.388425      -6.119522   \n",
       "75%          9.978733       24.674629      -58.375332      13.583131   \n",
       "max         73.737137       82.416367       46.695574      87.999207   \n",
       "\n",
       "         Index Tip y    Index Tip z  Middle Proximal x  Middle Proximal y  \\\n",
       "count  106979.000000  106979.000000      106979.000000      106979.000000   \n",
       "mean      -10.208400     -73.972310           0.712549           9.388218   \n",
       "std        40.269171      25.337680           8.255159           6.050276   \n",
       "min      -102.978952    -112.462585         -24.145329         -21.980112   \n",
       "25%       -45.302944     -93.672626          -6.837919           5.863414   \n",
       "50%       -14.418533     -80.410423           2.151676           9.575974   \n",
       "75%        23.936172     -60.066422           7.391723          13.382727   \n",
       "max        96.914932      53.381760          23.445007          25.185989   \n",
       "\n",
       "       Middle Proximal z  Middle Intermediate x  Middle Intermediate y  \\\n",
       "count      106979.000000          106979.000000          106979.000000   \n",
       "mean          -19.306369               9.581468               4.679483   \n",
       "std             3.587558              22.631948              25.144259   \n",
       "min           -25.657837             -54.249256             -65.532608   \n",
       "25%           -21.728305             -11.149257             -16.099387   \n",
       "50%           -20.124603              15.232921               4.461868   \n",
       "75%           -17.935727              27.901146              25.355267   \n",
       "max             6.137421              66.375984              69.576004   \n",
       "\n",
       "       Middle Intermediate z  Middle Distal x  Middle Distal y  \\\n",
       "count          106979.000000    106979.000000    106979.000000   \n",
       "mean              -53.065222        12.692946        -4.684326   \n",
       "std                10.117039        30.242326        37.583324   \n",
       "min               -72.724304       -75.109543       -91.424866   \n",
       "25%               -60.206032       -13.197766       -38.103338   \n",
       "50%               -54.737091        17.435110        -7.066628   \n",
       "75%               -48.052736        37.783436        27.261452   \n",
       "max                25.039421        88.228264        94.551773   \n",
       "\n",
       "       Middle Distal z   Middle Tip x   Middle Tip y   Middle Tip z  \\\n",
       "count    106979.000000  106979.000000  106979.000000  106979.000000   \n",
       "mean        -67.946889      13.756485     -12.317275     -74.908446   \n",
       "std          18.901129      34.760672      44.641463      27.063839   \n",
       "min        -100.599316     -91.211243    -109.174942    -118.985252   \n",
       "25%         -81.200862     -14.475380     -51.520982     -94.599333   \n",
       "50%         -72.400841      15.876219     -17.552639     -82.020462   \n",
       "75%         -59.167010      43.332731      25.106003     -60.922623   \n",
       "max          44.773242     102.207489     111.433588      55.553363   \n",
       "\n",
       "       Ring Proximal x  Ring Proximal y  Ring Proximal z  Ring Intermediate x  \\\n",
       "count    106979.000000    106979.000000    106979.000000        106979.000000   \n",
       "mean         17.529791         8.095233       -10.276684            25.648988   \n",
       "std           5.532313         7.853489         8.013560            18.961775   \n",
       "min         -12.639398       -25.039429       -26.869192           -40.555109   \n",
       "25%          13.344295         2.828438       -17.730928            10.075094   \n",
       "50%          18.373080         8.872391        -9.174782            29.783724   \n",
       "75%          22.278203        14.082314        -4.153049            41.117867   \n",
       "max          26.836068        25.465968        18.247375            65.926038   \n",
       "\n",
       "       Ring Intermediate y  Ring Intermediate z  Ring Distal x  Ring Distal y  \\\n",
       "count        106979.000000        106979.000000  106979.000000  106979.000000   \n",
       "mean             -0.778659           -40.999281      27.275005     -12.593958   \n",
       "std              22.917322            13.537246      27.110144      33.080043   \n",
       "min             -60.490051           -66.913315     -59.266098     -81.102333   \n",
       "25%             -19.526822           -51.560223       6.909097     -40.784522   \n",
       "50%              -2.790802           -41.144341      28.554492     -17.606827   \n",
       "75%              16.907257           -32.404678      50.301309      12.930918   \n",
       "max              64.414276            41.517410      90.692032      86.780737   \n",
       "\n",
       "       Ring Distal z     Ring Tip x     Ring Tip y     Ring Tip z  \\\n",
       "count  106979.000000  106979.000000  106979.000000  106979.000000   \n",
       "mean      -54.669632      27.006531     -21.898478     -60.856092   \n",
       "std        21.107323      32.277698      38.685426      28.408468   \n",
       "min       -93.231049     -75.195114     -92.351674    -111.118563   \n",
       "25%       -69.958648       3.556110     -54.368904     -81.196796   \n",
       "50%       -56.429403      24.772669     -29.923187     -64.962646   \n",
       "75%       -43.088371      54.606854       6.906043     -45.007335   \n",
       "max        58.788498     107.192108     102.598709      70.328949   \n",
       "\n",
       "       Pinky Proximal x  Pinky Proximal y  Pinky Proximal z  \\\n",
       "count     106979.000000     106979.000000     106979.000000   \n",
       "mean          32.352225          2.837640         -1.285509   \n",
       "std            4.083498         11.077100         13.504016   \n",
       "min          -30.062690        -36.622925        -36.324131   \n",
       "25%           30.378026         -4.798233        -14.394444   \n",
       "50%           33.172597          3.786240          2.370359   \n",
       "75%           35.187468         10.987781          9.982524   \n",
       "max           39.572960         34.939362         35.891083   \n",
       "\n",
       "       Pinky Intermediate x  Pinky Intermediate y  Pinky Intermediate z  \\\n",
       "count         106979.000000         106979.000000         106979.000000   \n",
       "mean              41.970822             -4.485664            -24.370822   \n",
       "std               14.765910             20.311902             18.142483   \n",
       "min              -37.905338            -56.838379            -66.809570   \n",
       "25%               32.979874            -20.168663            -40.403976   \n",
       "50%               43.076133             -7.020432            -22.701141   \n",
       "75%               54.273039             11.055683             -9.839353   \n",
       "max               69.175903             53.101288             52.591827   \n",
       "\n",
       "       Pinky Distal x  Pinky Distal y  Pinky Distal z    Pinky Tip x  \\\n",
       "count   106979.000000   106979.000000   106979.000000  106979.000000   \n",
       "mean        43.496761      -12.840841      -34.176921      42.845118   \n",
       "std         21.859076       26.318486       21.132027      28.005897   \n",
       "min        -43.796612      -71.155746      -80.829399     -53.352108   \n",
       "25%         28.580751      -33.926491      -51.121618      22.435043   \n",
       "50%         43.237972      -18.004105      -32.667877      41.146488   \n",
       "75%         62.777967        6.611061      -18.714948      67.228989   \n",
       "max         85.342997       67.743378       65.750656     101.063904   \n",
       "\n",
       "         Pinky Tip y    Pinky Tip z  \n",
       "count  106979.000000  106979.000000  \n",
       "mean      -21.385414     -40.369203  \n",
       "std        30.720860      25.151496  \n",
       "min       -81.252701     -96.391819  \n",
       "25%       -45.941994     -58.652584  \n",
       "50%       -28.819870     -38.904312  \n",
       "75%        -0.268571     -23.747971  \n",
       "max        81.727936      76.889313  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "The final pipeline will pass the data into an LSTM and then into an autoencoder to expand features into the 63 point space. This autoencoder will be trained first below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106979, 18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_ar = df.loc[:, 'ch1':'ch8'].values\n",
    "label_ar = df.loc[:, 'Wrist x':].values\n",
    "label_tips = label_ar[:,[0,1,2,12,13,14,24,25,26,36,37,38,48,49,50,60,61,62]]\n",
    "label_tips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106979, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106979, 63)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ar.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the autoencoder structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 63)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 153       \n",
      "_________________________________________________________________\n",
      "decoder_0 (Dense)            (None, 16)                160       \n",
      "_________________________________________________________________\n",
      "decoder_1 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "decoder_output (Dense)       (None, 63)                2079      \n",
      "=================================================================\n",
      "Total params: 5,512\n",
      "Trainable params: 5,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 9 #dimensionality of 'feature' vector\n",
    "\n",
    "a_fn = None\n",
    "\n",
    "# Full autoencoder\n",
    "input_vec = Input(shape=(63,))\n",
    "dense_0 = Dense(32, activation=a_fn)(input_vec)\n",
    "dense_1 = Dense(16, activation=a_fn)(dense_0)\n",
    "encoded = Dense(encoding_dim, activation=a_fn)(dense_1)\n",
    "dense_2 = Dense(16, activation=a_fn, name='decoder_0')(encoded)\n",
    "dense_3 = Dense(32, activation=a_fn, name='decoder_1')(dense_2)\n",
    "decoded = Dense(63, activation=a_fn, name='decoder_output')(dense_3)\n",
    "\n",
    "autoencoder = Model(input_vec, decoded)\n",
    "\n",
    "# Encoder from autoencoder\n",
    "encoder = Model(input_vec, encoded)\n",
    "\n",
    "# Decoder from autoencoder layers\n",
    "decoder_input = Input(shape=(encoding_dim,), name='encoded_input')\n",
    "decode_0 = autoencoder.layers[-3](decoder_input)\n",
    "decode_1 = autoencoder.layers[-2](decode_0)\n",
    "decode_output = autoencoder.layers[-1](decode_1)\n",
    "decoder = Model(decoder_input, decode_output, name='decoder')\n",
    "\n",
    "# Train Autoencoder\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 18)]              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 9)                 171       \n",
      "_________________________________________________________________\n",
      "decoder_output (Dense)       (None, 18)                180       \n",
      "=================================================================\n",
      "Total params: 351\n",
      "Trainable params: 351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 9 #dimensionality of 'feature' vector\n",
    "\n",
    "a_fn = None\n",
    "\n",
    "# Full autoencoder\n",
    "input_vec = Input(shape=(18,))\n",
    "#dense_0 = Dense(9, activation=a_fn)(input_vec)\n",
    "#dense_1 = Dense(5, activation=a_fn)(dense_0)\n",
    "encoded = Dense(encoding_dim, activation=a_fn)(input_vec)\n",
    "#dense_2 = Dense(5, activation=a_fn, name='decoder_0')(encoded)\n",
    "#dense_3 = Dense(16, activation=a_fn, name='decoder_1')(dense_2)\n",
    "decoded = Dense(18, activation=a_fn, name='decoder_output')(encoded)\n",
    "\n",
    "autoencoder_tips = Model(input_vec, decoded)\n",
    "\n",
    "# Encoder from autoencoder\n",
    "encoder = Model(input_vec, encoded)\n",
    "\n",
    "# Decoder from autoencoder layers\n",
    "decoder_input = Input(shape=(encoding_dim,), name='encoded_input')\n",
    "#decode_0 = autoencoder.layers[-3](decoder_input)\n",
    "#decode_1 = autoencoder.layers[-2](decode_0)\n",
    "decode_output = autoencoder.layers[-1](decoder_input)\n",
    "decoder = Model(decoder_input, decode_output, name='decoder')\n",
    "\n",
    "# Train Autoencoder\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "autoencoder_tips.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "autoencoder_tips.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.evaluate(label_ar, label_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6631.058497845517"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_tips.evaluate(label_tips, label_tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above evaluation the initial untrained loss is around 2500.\n",
    "Now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = autoencoder.fit(label_ar, label_ar, batch_size=312, epochs=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85583 samples, validate on 21396 samples\n",
      "Epoch 1/10\n",
      "85583/85583 [==============================] - 1s 7us/sample - loss: 24.5897 - val_loss: 31.2787\n",
      "Epoch 2/10\n",
      "85583/85583 [==============================] - 1s 7us/sample - loss: 24.3930 - val_loss: 31.0273\n",
      "Epoch 3/10\n",
      "85583/85583 [==============================] - 1s 6us/sample - loss: 24.2394 - val_loss: 30.8665\n",
      "Epoch 4/10\n",
      "85583/85583 [==============================] - 1s 6us/sample - loss: 24.1134 - val_loss: 30.4065\n",
      "Epoch 5/10\n",
      "85583/85583 [==============================] - 1s 7us/sample - loss: 24.0083 - val_loss: 30.4927\n",
      "Epoch 6/10\n",
      "85583/85583 [==============================] - 1s 7us/sample - loss: 23.9261 - val_loss: 30.2513\n",
      "Epoch 7/10\n",
      "85583/85583 [==============================] - 1s 8us/sample - loss: 23.8553 - val_loss: 30.1568\n",
      "Epoch 8/10\n",
      "85583/85583 [==============================] - 1s 7us/sample - loss: 23.8007 - val_loss: 30.0027\n",
      "Epoch 9/10\n",
      "85583/85583 [==============================] - 1s 6us/sample - loss: 23.7578 - val_loss: 29.9771\n",
      "Epoch 10/10\n",
      "85583/85583 [==============================] - 1s 7us/sample - loss: 23.7138 - val_loss: 29.7815\n"
     ]
    }
   ],
   "source": [
    "ret = autoencoder_tips.fit(label_tips, label_tips, batch_size=312, epochs=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the encoder - decoder pair will be tested seperately for loss: 20 aprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5556.819701115771"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_vec = encoder.predict(label_tips)\n",
    "decoded_vec = decoder.predict(encoded_vec)\n",
    "\n",
    "#compilation is only to enable evaluation\n",
    "decoder.compile(optimizer='adam', loss='mse')\n",
    "decoder.evaluate(encoded_vec, label_tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106979, 9)\n",
      "(106979, 18)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_vec.shape)\n",
    "print(decoded_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "Here the lstm will be trained. The 8 channels of sEMG data will be processed by the LSTM, which will return a vector of 9 complex features. These will then be processed by the autoencoder, trained in the previous section, to create the 63 hand coordinates.\n",
    "\n",
    "To train the sequential recurrent LSTM the data will be grouped into 'sequences' of 24 steps in time. The LSTM will be trained with many sequence groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106956, 24, 8)\n",
      "(106956, 63)\n",
      "(106956, 18)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 24\n",
    "\n",
    "def overlap_samples(seq_length, feats, labels):\n",
    "    new_l = labels[seq_length - 1:]\n",
    "    feat_list = [feats[i:i + seq_length] for i in range(feats.shape[0] - seq_length + 1)]\n",
    "    new_f = np.array(feat_list)\n",
    "    return new_f, new_l\n",
    "\n",
    "features, labels = overlap_samples(seq_length, feature_ar, label_ar)\n",
    "features, labels_tips = overlap_samples(seq_length, feature_ar, label_tips)\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "print(labels_tips.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#lstm layers\n",
    "inputs = Input(shape=(None, 8), name=\"inputs\")\n",
    "lstm_0 = LSTM(64, return_sequences=True, name=\"lstm_0\")(inputs)\n",
    "do = Dropout(0.2)(lstm_0)\n",
    "lstm_1 = LSTM(64, return_sequences=False, name=\"lstm_1\")(do)\n",
    "do_2 = Dropout(0.2)(lstm_1)\n",
    "lstm_out = Dense(63, activation=None, name=\"lstm_out\")(do_2)\n",
    "\n",
    "#decoder layers\n",
    "#decoder_0 = decoder.get_layer(\"decoder_0\")(lstm_out)\n",
    "#decoder_0.trainable = False\n",
    "#decoder_1 = decoder.get_layer(\"decoder_1\")(decoder_0)\n",
    "#decoder_1.trainable = False\n",
    "#decoder_output = decoder.get_layer(\"decoder_output\")(decoder_1)\n",
    "#decoder_output.trainable = False\n",
    "\n",
    "model = Model(inputs, lstm_out, name=\"model_v1\")\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00, amsgrad=False)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_v1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, None, 8)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_0 (LSTM)                (None, None, 32)          5248      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 32)          8320      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, None, 32)          8320      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, None, 32)          8320      \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, None, 32)          8320      \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, None, 32)          8320      \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 9)                 297       \n",
      "_________________________________________________________________\n",
      "decoder_output (Dense)       (None, 18)                180       \n",
      "=================================================================\n",
      "Total params: 56,541\n",
      "Trainable params: 56,093\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#lstm layers\n",
    "inputs = Input(shape=(None, 8), name=\"inputs\")\n",
    "model = LSTM(32, return_sequences=True, name='lstm_0')(inputs)\n",
    "model = Dropout(0.2)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = LSTM(32, return_sequences=True, name='lstm_1')(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = LSTM(32, return_sequences=True)(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = LSTM(32, return_sequences=True)(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = LSTM(32, return_sequences=True)(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = LSTM(32, return_sequences=True)(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = LSTM(32, return_sequences=False)(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = Dense(9, activation=None)(model)\n",
    "\n",
    "#decoder layers\n",
    "#decoder_0 = decoder.get_layer(\"decoder_0\")(model)\n",
    "#decoder_0.trainable = False\n",
    "#decoder_1 = decoder.get_layer(\"decoder_1\")(decoder_0)\n",
    "#decoder_1.trainable = False\n",
    "#decoder_output = decoder.get_layer(\"decoder_output\")(decoder_1)\n",
    "decoder_output = decoder.get_layer(\"decoder_output\")(model)\n",
    "decoder_output.trainable = False\n",
    "\n",
    "model = Model(inputs, decoder_output, name=\"model_v1\")\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00, amsgrad=False)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2383.4007993247515"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(features, labels_tips, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_weights = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85564 samples, validate on 21392 samples\n",
      "Epoch 1/4\n",
      "85564/85564 [==============================] - 320s 4ms/sample - loss: 596.2451 - val_loss: 830.7177\n",
      "Epoch 2/4\n",
      "85564/85564 [==============================] - 295s 3ms/sample - loss: 576.3131 - val_loss: 813.1110\n",
      "Epoch 3/4\n",
      "85564/85564 [==============================] - 305s 4ms/sample - loss: 558.5094 - val_loss: 790.9144\n",
      "Epoch 4/4\n",
      "85564/85564 [==============================] - 330s 4ms/sample - loss: 541.7303 - val_loss: 806.8447\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(features, labels_tips, batch_size=seq_length, epochs=4, verbose=1, validation_split=0.2,\n",
    "                    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save('AE_jose_all_fingers_val_loss_500_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the validation loss over the training epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXxU1fn48c8zk8lKSEIIiwn7mgiKkCKWolJBWWzx26rFn1Vc+aEoVr9Wsba1VfuTqq/6BTe+WFGprWi1KsoOFbUqYsAomLCEPUggBAhLErKd3x/3AkMWssxM7kzmeb9e88qdc8/MPMfB59x77plzxRiDUkqp8OByOgCllFItR5O+UkqFEU36SikVRjTpK6VUGNGkr5RSYSTC6QDOpn379qZ79+5Oh6GUUiFl7dq1B4wxKXXtC+qk3717d7KyspwOQymlQoqI7Kxvnw7vKKVUGNGkr5RSYUSTvlJKhZGgHtNXSqmmqKioID8/n7KyMqdDaRHR0dGkpaXh8Xga/RpN+kqpViM/P5/4+Hi6d++OiDgdTkAZYygqKiI/P58ePXo0+nU6vKOUajXKyspITk5u9QkfQERITk5u8lmNJn2lVKsSDgn/pOa0tVUm/aNlFTy5ZCM7i447HYpSSgWVVpn0S8qrePXzHcxYvNHpUJRSYaSoqIhBgwYxaNAgOnXqRGpq6qnn5eXljXqPm2++mU2bNgUsxlZ5Ibdj22imXNKLvyzfzJfbiriwZ7LTISmlwkBycjLZ2dkA/OEPf6BNmzbcf//9Z9QxxmCMweWq+5j7lVdeCWiMrfJIH+D2ET3pnBDN4wtzqa7Wu4MppZyTl5dHRkYG119/Peeeey579+5l8uTJZGZmcu655/Loo4+eqvujH/2I7OxsKisrSUxMZPr06Zx//vlcdNFF7N+/3+dYWuWRPkBMpJsHxvTj3je/4b3sPfxscJrTISmlWtAfP/iOnO+P+PU9M85pyyM/ObdZr924cSPz5s0jMzMTgBkzZtCuXTsqKysZOXIkV199NRkZGWe8pri4mEsuuYQZM2Zw3333MXfuXKZPn+5TGxo80heRuSKyX0Q2eJUNEpHVIpItIlkiMtQuFxGZJSJ5IvKtiAz2es0kEdliPyb5FHUjTTg/lfPSEnhyySZKyitb4iOVUqpOvXr1OpXwAd544w0GDx7M4MGDyc3NJScnp9ZrYmJiGDt2LABDhgxhx44dPsfRmCP9V4HngHleZU8CfzTGLBaRcfbzS4GxQB/7cSHwInChiLQDHgEyAQOsFZEFxphDPrfgLFwu4XdXZnDN7C946ZPt3DOqTyA/TikVRJp7RB4ocXFxp7a3bNnCzJkzWbNmDYmJifzyl7+sc759ZGTkqW23201lpe8Hrw0e6RtjPgEO1iwG2trbCcD39vYEYJ6xrAYSRaQzcAWw3Bhz0E70y4ExPkffCD/o3o5xAzsx++OtFBSHx0+zlVLB7ciRI8THx9O2bVv27t3L0qVLW+yzm3sh91fAUyKyG3gaeMguTwV2e9XLt8vqK69FRCbbQ0ZZhYWFzQzvTNPHpFNVbXh6WeCmQSmlVGMNHjyYjIwM+vfvz4033sjw4cNb7LPFmIZntohId+BDY8wA+/ks4GNjzDsici0w2RgzSkQ+BGYYY/5j11sJPIg19BNtjHncLv8dUGqMefpsn5uZmWn8dROVJxblMufTbXxw148YkJrgl/dUSgWX3Nxc0tPTnQ6jRdXVZhFZa4zJrKt+c4/0JwH/srf/CQy1t/cAXbzqpdll9ZW3mKk/7k1SbCSPfZhDYzo6pZRqjZqb9L8HLrG3fwxssbcXADfas3iGAcXGmL3AUuByEUkSkSTgcrusxbSN9nDv6L58uf0gy3L2teRHK6VU0Ghw9o6IvIE1PNNeRPKxZuHcDswUkQigDJhsV18EjAPygBLgZgBjzEEReQz4yq73qDGm5sXhgLvuB12Y9/kOnliUy8h+HYiMaLW/TVNKqTo1mPSNMdfVs2tIHXUNMLWe95kLzG1SdH4W4Xbx8Ph0bnrlK+Z9sYPbRvR0MhyllGpxYXeoe2m/DlzcN4VZK7dw6HjjFkBSSqnWIuySPsDD49I5dqKSmSu3NFxZKaVakbBM+v06xXPd0K78bfVO8vYfczocpVQr4Y+llQHmzp1LQUFBQGIMy6QPcO/ovsR63DyxKNfpUJRSrcTJpZWzs7OZMmUK995776nn3ksqNESTfgC0bxPF1B/3ZuXG/fxnywGnw1FKtXKvvfYaQ4cOZdCgQdx5551UV1dTWVnJDTfcwMCBAxkwYACzZs3izTffJDs7m1/84hdNPkNojFa7tHJj3PTD7ry+eiePL8xh4bQRuF3hc29NpVq9xdOhYL1/37PTQBg7o8kv27BhA++++y6ff/45ERERTJ48mfnz59OrVy8OHDjA+vVWnIcPHyYxMZFnn32W5557jkGDBvk3fsL4SB8g2uNm+tj+bCw4yj+zdjf8AqWUaoYVK1bw1VdfkZmZyaBBg/j444/ZunUrvXv3ZtOmTUybNo2lS5eSkBD4JWLC+kgfYPzAzrzSbQdPL9vMleefQ5uosP9PolTr0Iwj8kAxxnDLLbfw2GOP1dr37bffsnjxYp5//nneeecd5syZE9BYwvpIH0DEWnP/wLETvLgqz+lwlFKt0KhRo3jrrbc4cMC6flhUVMSuXbsoLCzEGMM111zDo48+yrp16wCIj4/n6NGjAYlFD2uBQV0SuWrQObz06XauG9qVtKRYp0NSSrUiAwcO5JFHHmHUqFFUV1fj8XiYPXs2brebW2+9FWMMIsKf//xnAG6++WZuu+02YmJiWLNmTZNm/jSkUUsrO8WfSys3ZM/hUn789CrGDOjEzIkXtMhnKqX8S5dWtgRiaeVWJzUxhttH9OT97O/5eldA7+KolFKO0aTv5Y5Le5ESH6Vr7iulWi1N+l7ioiK4//K+rNt1mA+/3et0OEqpZginA7bmtFWTfg1XD+lCeue2zFi8kbKKKqfDUUo1QXR0NEVFRWGR+I0xFBUVER0d3aTX6eydGtwu4bfj07n+r18y97Pt3Hlpb6dDUko1UlpaGvn5+RQWFjodSouIjo4mLS2tSa/RpF+H4b3bMyq9Ay98tJVrhnQhJT7K6ZCUUo3g8Xjo0aOH02EENR3eqcdD49Ipq6jimRWbnQ5FKaX8RpN+PXqltOGXw7oxf80uNhYccTocpZTyC036Z/GrUX2Ij/bwp4W5YXFhSCnV+mnSP4vE2EimXdaHT7ccYNWm8LgwpJRq3TTpN+CGYd3o0T6OxxfmUFFV7XQ4SinlE036DYiMcPHQ2P5sLTzO/DW7nA5HKaV8okm/EUZndGRYz3b8ZflmiksrnA5HKaWarcGkLyJzRWS/iGyoUX63iGwUke9E5Emv8odEJE9ENonIFV7lY+yyPBGZ7t9mBJaI8NvxGRwureD5j3TNfaVU6GrMkf6rwBjvAhEZCUwAzjfGnAs8bZdnABOBc+3XvCAibhFxA88DY4EM4Dq7bsgYkJrA1YPTeOWz7ewsOu50OEop1SwNJn1jzCfAwRrFdwAzjDEn7Dr77fIJwHxjzAljzHYgDxhqP/KMMduMMeXAfLtuSLn/in543C5mLN7odChKKdUszR3T7wuMEJEvReRjEfmBXZ4KeN9hPN8uq6+8FhGZLCJZIpIVbOtndGwbzZRLerF4QwFrttfsB5VSKvg1N+lHAO2AYcCvgbdERPwRkDFmjjEm0xiTmZKS4o+39KvbR/Skc0I0j32YQ3W1/mBLKRVampv084F/GcsaoBpoD+wBunjVS7PL6isPOTGRbh4Y04/1e4p5Lzskm6CUCmPNTfrvASMBRKQvEAkcABYAE0UkSkR6AH2ANcBXQB8R6SEikVgXexf4GrxTJpyfynlpCTy5ZBMl5ZVOh6OUUo3WmCmbbwBfAP1EJF9EbgXmAj3taZzzgUn2Uf93wFtADrAEmGqMqTLGVAJ3AUuBXOAtu25IcrmE312ZQcGRMl76ZLvT4SilVKNJMC8klpmZabKyspwOo153/n0tH20sZNWvL6Vj26bdvUYppQJFRNYaYzLr2qe/yPXB9DHpVFUbnlq6yelQlFKqUTTp+6Brciw3D+/OO+vy2bCn2OlwlFKqQZr0fTT1x71Jio3ksQ9zdM19pVTQ06Tvo7bRHu4d3Zcvtx9kWc4+p8NRSqmz0qTvB9f9oAu9O7ThiUW5lFfqmvtKqeClSd8PItwuHh6fzo6iEuZ9scPpcJRSql6a9P1kZL8OXNw3hVkrt3DoeLnT4SilVJ006fvRw+PSOXaikpkrtzgdilJK1UmTvh/16xTPdUO78rfVO8nbf8zpcJRSqhZN+n527+i+xHjczFic63QoSilViyZ9P2vfJoqpI3uzInc/n+UdcDocpZQ6gyb9ALh5eHfSkmJ47MMcqnTNfaVUENGkHwDRHjfTx/ZnY8FR/pm1u+EXKKVUC9GkHyDjB3ZmSLcknl62mWMndM19pVRw0KQfICLCb8enc+DYCWav2up0OEopBWjSD6gLuiYxYdA5vPTpNvYcLnU6HKWU0qQfaA+M6Q/Ak0s2OhyJUkpp0g+41MQYbh/Rk/ezv+frXYecDkcpFeY06beAOy7tRUp8lK65r5RynCb9FhAXFcH9l/dl3a7DLFy/1+lwlFJhTJN+C7l6SBfSO7dlxuKNlFVUOR2OUipMadJvIW6XNYUz/1Apr3y2w+lwlFJhSpN+Cxreuz2j0jvw/Ed5FB494XQ4Sqkw1GDSF5G5IrJfRDbUse+/RcSISHv7uYjILBHJE5FvRWSwV91JIrLFfkzybzNCx0Pj0imrqOKZFZudDkUpFYYac6T/KjCmZqGIdAEuB3Z5FY8F+tiPycCLdt12wCPAhcBQ4BERSfIl8FDVK6UNvxzWjflrdrGx4IjT4SilwkyDSd8Y8wlwsI5dzwAPAN5zECcA84xlNZAoIp2BK4DlxpiDxphDwHLq6EjCxa9G9SE+2sOfFubqFE6lVItq1pi+iEwA9hhjvqmxKxXwXlYy3y6rr7yu954sIlkiklVYWNic8IJeYmwk0y7rw6dbDrBqc+tso1IqODU56YtILPAb4Pf+DweMMXOMMZnGmMyUlJRAfERQuGFYN3q0j+NPC3OpqKp2OhylVJhozpF+L6AH8I2I7ADSgHUi0gnYA3Txqptml9VXHrYiI1w8NLY/efuPMX/NroZfoJRSftDkpG+MWW+M6WCM6W6M6Y41VDPYGFMALAButGfxDAOKjTF7gaXA5SKSZF/AvdwuC2ujMzoyrGc7/rJ8M8WlFU6Ho5QKA42ZsvkG8AXQT0TyReTWs1RfBGwD8oCXgDsBjDEHgceAr+zHo3ZZWLPW3M/gcGkFz3+U53Q4SqkwENFQBWPMdQ3s7+61bYCp9dSbC8xtYnyt3oDUBK4enMarn+3g+gu70i05zumQlFKtmP4iNwjcf0U/ItzCjMW65r5SKrA06QeBjm2jmXJJLxZvKGDN9rAf9VJKBZAm/SBx+4iedE6I5rEPc6iu1h9sKaUCQ5N+kIiJdPPAmH6s31PMe9lhPZtVKRVAmvSDyITzUzkvLYEnl2yitFzX3FdK+Z8m/SDicllTOAuOlDHnk21Oh6OUaoU06QeZoT3aMW5gJ2Z/vJV9R8qcDkcp1cpo0g9C08ekU1VteGrpJqdDUUq1Mpr0g1DX5FhuHt6dd9bls2FPsdPhKKVaEU36QerOkb1Jio3k8YU5uua+UspvNOkHqYQYD/eO6sPqbQdZlrPP6XCUUq2EJv0gdt3QrvTu0IYnFuVSXqlr7iulfKdJP4hFuF08PD6dHUUlzPtih9PhKKVaAU36QW5kvw5c3DeFWSu3cOh4udPhKKVCnCb9EPDwuHSOnahk5sotToeilApxmvRDQL9O8Uwc2pXXV+9ka+Exp8NRSoUwTfoh4r7RfYn2uHliUa7ToSilQpgm/RDRvk0UU0f2ZkXufj7LO+B0OEqpEKVJP4TcPLw7aUkxPPZhDlW65r5Sqhk06YeQaI+b6WP7s7HgKG+v3e10OEqpEKRJP8SMH9iZId2SeGrpZo6dqHQ6HKVUiGm9Sf9g61yPXkT47fh0Dhw7wexVW50ORykVYlpn0j+8C54fBq//HPZ953Q0fndB1yQmDDqHlz7dxp7DpU6Ho5QKIQ0mfRGZKyL7RWSDV9lTIrJRRL4VkXdFJNFr30Mikicim0TkCq/yMXZZnohM939TvLTpCJf9DvK/gtk/gvenwpHvA/qRLe2BMf0BeHLJRocjUUqFksYc6b8KjKlRthwYYIw5D9gMPAQgIhnAROBc+zUviIhbRNzA88BYIAO4zq4bGBFR8MO7YVo2DLsTvn0LZg2Gfz8OJ44G7GNbUmpiDLeP6Mn72d/z9a5DToejlAoRDSZ9Y8wnwMEaZcuMMSevIq4G0uztCcB8Y8wJY8x2IA8Yaj/yjDHbjDHlwHy7bmDFtoMr/gRT10C/sfDJUzDrAvjqZagK/Yugd1zai5T4KB5fmKtr7iulGsUfY/q3AIvt7VTAey5hvl1WX3ktIjJZRLJEJKuwsNAP4QHtesA1r8Bt/4bkPrDwPnjxh7BpMYRwsoyLiuD+y/uyduchFq7f63Q4SqkQ4FPSF5GHgUrg7/4JB4wxc4wxmcaYzJSUFH+9rSVtCNy8CCb+A0wVvDERXr0S9qzz7+e0oKuHdCG9c1tmLN5IWUWV0+EopYJcs5O+iNwEXAlcb06PLewBunhVS7PL6itveSLQfzzcuRrGPQ2FG+GlkfDObXBopyMh+cLtsqZw5h8q5ZXPdjgdjlIqyDUr6YvIGOAB4KfGmBKvXQuAiSISJSI9gD7AGuAroI+I9BCRSKyLvQt8C91Hbg8MvR2mfQ0j/htyP4DnMmHZb6E0tC6MDu/dnlHpHXj+ozwKj55wOhylVBBrzJTNN4AvgH4iki8itwLPAfHAchHJFpHZAMaY74C3gBxgCTDVGFNlX/S9C1gK5AJv2XWdF90WLvs93L0OBl4Dnz9nXez94gWoDJ2bljw0Lp2yiiqeWbHZ6VCUUkFMgnnWR2ZmpsnKymrZDy1YD8t+B9s+gqTuMOoPkHGVNSwU5P6w4DvmfbGDxfdcTL9O8U6Ho5RyiIisNcZk1rWvdf4i1xedBsKN78Ev3wFPLPzzJnh5NOxa7XRkDbrnsj7ER3t4fGGOTuFUStVJk359eo+CKf+Bnz4Hxfkw9wqYfz0cyHM6snolxUUy7bI+fLrlAKs2+2m6q1KqVdGkfzYuNwy+Ae5eCyMfhm2r4IULYdGv4Xhw3sjkhmHd6NE+jj8tzKWiqtrpcJRSQUaTfmNExsElD1gzfQbfaP2id9YF8OlfoCK4FjyLjHDx0Nj+5O0/xvw1u5wORykVZDTpN0WbDnDlM3DnF9BtOKz8IzybCdlvQHXwHFWPzujIsJ7teGbFFopLK5wORykVRDTpN0dKP/g/82HShxDXHt6bAnMusYZ/goC15n4Gh0rKef6j4L0GoZRqeZr0fdFjBNz+Efzsr1B6GOZNgNevhn05TkfGgNQErh6cxquf7WBn0XGnw1FKBQlN+r5yueC8a+Cur2D0Y7B7DcweDgvuhqMFjoZ2/xX9iHALMxbrmvtKKYsmfX/xRMPwaXBPNlw4xRrnn3UBfPQEnDjmSEgd20Yz5ZJeLN5QwJrtBxt+gVKq1dOk72+x7WDME3DXGuh7BXw8A54dDFmvOLKG/+0jetKpbTSPL8yhulp/sKVUuNOkHyjtesI1r8KtKyCpB3z4K2vYZ/PSFl3DPybSzQNj+vFtfjHvZTuzsKlSKnho0g+0Lj+AW5bAtX+Dqgr4x7Xw2k/g++wWC+GqQamcl5bAk0s2UVqua+4rFc406bcEEcj4KUz9EsY+BftzrCme79wOhwP/AyqXy5rCWXCkjDmfbAv45ymlgpcm/Zbk9sCFk61f9v7oXshdYP24a/nvrSmfATS0RzvGDezE7I+3su9IWUA/SykVvDTpOyE6wVqy+e61MOBn8Nksa6bP6tkBXcP/wTH9qao2PL10U8A+QykV3DTpOykhDf5rNvzfj60lnZc8aC3o9t17AbnY2y05jpuGd+ftdfls2FPs9/dXSgU/TfrBoPP5cOP7cP3b4I6Cf06Cly+HXV/6/aOmjuxNUmykrrmvVJjSpB8sRKDPaGsN/5/Msi7wzr0c3rwBirb67WMSYjzcO6oPq7cdZFnOPr+9r1IqNGjSDzbuCBgyCaatg0t/A3kr4fkLYfGDcLzILx9x3dCu9O7QhicW5VJeGTyrgyqlAk+TfrCKjINLH7Rm+lxwPayZY13s/c//QIVvs28i3C4eHp/OjqIS/rZ6p58CVkqFAk36wS6+I/xkJtzxOXQdBisegecy4Zs3fVrDf2S/DlzcN4WZKzZz6HjgZgwppYKLJv1Q0SEdrn8LJn1gre/z7mR46VLY9nGz3/LhcekcO1HJzJVb/BenUiqoadIPNT0uhttXwc9egpKDMO+n8PdrYX/Tl0/u1ymeiUO78vrqnWwtdGYlUKVUy9KkH4pcLjjvWrgrC0b9EXathhcvgg/ugaNNm5Fz3+i+RHvcPLEoN0DBKqWCSYNJX0Tmish+EdngVdZORJaLyBb7b5JdLiIyS0TyRORbERns9ZpJdv0tIjIpMM0JM55o+NGvrIu9QyfD169bF3tXzYDyxt0tq32bKKaO7M2K3P18lncgwAErpZzWmCP9V4ExNcqmAyuNMX2AlfZzgLFAH/sxGXgRrE4CeAS4EBgKPHKyo1B+EJcMY/8MU9dAn1Gw6gmYNRjWvgbVDa+qefPw7qQlxfDYhzlU6Zr7SrVqDSZ9Y8wnQM3bLk0AXrO3XwOu8iqfZyyrgUQR6QxcASw3xhw0xhwCllO7I1G+Su4F186DW5ZBYlf4YBq8OBw2Lzvrsg7RHjfTx/ZnY8FR3l67uwUDVkq1tOaO6Xc0xuy1twuAjvZ2KuCdNfLtsvrKaxGRySKSJSJZhYWFzQwvzHW9EG5dZnUAVSfgH9dYN23f+029Lxk/sDNDuiXx1NLNHDvR8nf4Ukq1DJ8v5BprARe/jQkYY+YYYzKNMZkpKSn+etvwIwIZE+DOL2HMn6FgPfzvJfDuFCjOr6O68Nvx6Rw4doLZq/y37INSKrg0N+nvs4dtsP/ut8v3AF286qXZZfWVq0CLiIRhU6yLvcPvgQ3/gmeHwIo/QNmZK21e0DWJCYPO4aVPt7HncKkz8SqlAqq5SX8BcHIGziTgfa/yG+1ZPMOAYnsYaClwuYgk2RdwL7fLVEuJSYTRf4S7s6wzgP88Y830+XKOdRtH2wNj+gPw5JKmz/tXSgW/xkzZfAP4AugnIvkiciswAxgtIluAUfZzgEXANiAPeAm4E8AYcxB4DPjKfjxql6mWltgVfjYHJq+CDhmw+NfWgm45C8AYUhNjuH1ET97P/p6vdx1yOlqllJ9JMK+pnpmZabKyspwOo/UyBrYss27XWLgRugyDyx/nWIcLGPn0Krq2i+XtKRchIk5HqpRqAhFZa4zJrGuf/iI3nIlA3ytgymfWom4Ht8HLo2jz/q08MjyGtTsPsXD93obfRykVMjTpK3sN/5usi72XTIctyxj/yQSeSXiTFxatoayi4R94KaVCgyZ9dVpUGxj5ENy9Dhl0HVeVf8D80jv4+o0/+ryGv1IqOGjSV7W17Qw/fRaZ8hk7Ygdw0baZVD2bCd/+06c1/JVSztOkr+rXMYO4W97lhoqH2VceDf+6DV4aCds/dToypVQzadJXZ9UrpQ29LhzPiOJH2Pvj/4HjB+C1K+EfE6Fwk9PhKaWaSJO+atA9l/WhTXQUD2zJwNz1FVz2COz8DF64CD68F47tb/hNlFJBQZO+alBSXCTTLuvDp1sOsGr7MRhxnzXT5we3wrp51i97P36y0Wv4K6Wco0lfNcoNw7rRPTmWPy3MpbKqGuLaw7inrAXdeo2Ej/5kremzbl6j1vBXSjlDk75qlMgIFw+NSydv/zHeWLPr9I72veEXr8MtSyEhDRbcDbNHQN4K54JVStVLk75qtMszOjKsZzueWbGF4tKKM3d2HQa3LodrXoWK4/D6z2HeVdaSzkqpoKFJXzWateZ+BodKynn+o7y6KsC5/2XdtvGKJ2BvtnXU/+4dUKwraSsVDDTpqyYZkJrA1YPTePWzHewsqufCbUQUXHSndbH3h3fDhrfh2cGw8lFryqeO+SvlGF1lUzXZviNlXPrUKkb2T+GF64c0/IJDO+Hfj8H6f54uc0dBZCx44sATc3o7MhY8sRAZZ/+tUV5X2am69t+IKOusQ6kwdbZVNiNaOhgV+jq2jWbKJb14ZsVm1mw/yNAe7c7+gqRu8PO/wkV3wY5PobzEGvcvL4GKEmuqZ0WJ9bykCMp3W89PllU28S5e4mp6ZxJp129MXZe7+f/xlHKYHumrZiktr2Lk06vo0DaK9+4cjssVwCPr6mqvTsCrg/DuOOoqa7CuXcc0cbgpIrrxHcSpzqSOM5K66roj9SxF+UyP9JXfxUS6eWBMP+576xvey97DzwanBe7DXC5rBdCoNv5/b2Ogqtw/nUnJAThco05lE1cnFbdXh9BQZxJrn800ojM5OTTm0st44U6Tvmq2qwal8urnO3hyySbGDuhMTGQIDnuIWNcAIqKABoapmqO66iwdR2mNTqSezqTc3j5+oHYd08RVTyOi6+4g3B77EQmuCB+2Pdb9GU5t24+zbkdarzm57XLr2U4AadJXzeZyWVM4r/3fL3jp021Mu6yP0yEFH5cbouKth78ZA5Un6ug4mtCZnKp7DKoqrEd1xdm3mzoc1hx1dQb1bnvsjqeB7QY7Hz91dEHeaWnSVz4Z2qMdYwd04sVVW/nFD7rQsW200yGFDxHwRFuP2ACcpdSnutqrMyiH6spGbFdAVWUjtuvrbMqtevVtV5Y1XOfkdlPPjpqjzrOeJp4BJfeGSx/0e2ia9JXPpo/tz8rc/Ty9dBNPXXO+0+GoQHO5wHVySCwEneq0yu1OpbLh7TM6osZ2dE3o9CpKa5dXlASk+Zr0lc+6Jcdx0/DuvPTpNib9sDsDUhOcDkmp+oV6p+UjvZSv/GLqyN4kxUby+MIcgsHhB8wAAAx7SURBVHkasFLhzqekLyL3ish3IrJBRN4QkWgR6SEiX4pInoi8KSKRdt0o+3mevb+7PxqggkNCjId7R/Vh9baDLM/Z53Q4Sql6NDvpi0gqMA3INMYMANzARODPwDPGmN7AIeBW+yW3Aofs8mfseqoVuW5oV3p3aMP/W5RLeaXeQF2pYOTr8E4EECMiEUAssBf4MfC2vf814Cp7e4L9HHv/ZSJBPK9JNVmE28XD49PZUVTC31bvdDocpVQdmp30jTF7gKeBXVjJvhhYCxw2xlTa1fKBVHs7Fdhtv7bSrp9c831FZLKIZIlIVmFhYXPDUw65tG8KI/q0Z+aKzRw6Xu50OEqpGnwZ3knCOnrvAZwDxAFjfA3IGDPHGJNpjMlMSUnx9e1UCzu55v6xE5XMXLnF6XCUUjX4MrwzCthujCk0xlQA/wKGA4n2cA9AGnDy7hl7gC4A9v4EoMiHz1dBql+neCYO7crrq3eytfCY0+Eopbz4kvR3AcNEJNYem78MyAE+Aq6260wC3re3F9jPsff/2+jcvlbrvtF9ifa4eWJRrtOhKKW8+DKm/yXWBdl1wHr7veYADwL3iUge1pj9y/ZLXgaS7fL7gOk+xK2CXPs2UUwd2ZsVufv5LO+A0+EopWy6nr4KmLKKKkb95WPaREWwcNoI3IFcc18pdcrZ1tPXX+SqgIn2uJk+tj8bC47y9trdToejlEKTvgqw8QM7M6RbEk8v28yxE5UNv0ApFVCa9FVAWVM40yk8eoLZq7Y6HY5SYU+Tvgq4C7omMWHQObz06Tb2HG7iTc6VUn6lSyurFvHAmP4s2VDAXf9Yx6j0jnRLjqVrO+uREONBV+RQqmVo0lctIjUxht9dmcH/rNjMU0s3nbEvPjqCru1i6ZYcS5d2pzuDru1iOScxBo9bT0iV8hedsqlaXEl5JbsPlrKz6Di7Dpaw+2AJuw6WsPNgCfkHSymvOr1Cp9slnJMYfaoT6NIulm7t4k6fJcR6HGyJUsHpbFM29UhftbjYyAj6dYqnX6faNwuvrjbsO1rGriKrI/B+LM/Zx4FjZy7i1jY6gm7Jcac6hJOdQbfkWDonRBOhZwlKnUGTvgoqLpfQOSGGzgkxXNiz1iKsHD9ReaoT2O3VIeTuPcLynH21zhJSE2PO6BBOXkvoYl9LUCrcaNJXISUuKoL0zm1J79y21r6qasO+I2VWR1DjTGHZdwUU1VjqOSHGY50ZJJ95HaFrOz1LUK2XJn3Valjj/zGckxjDsDrOEo6WVbD7YGmt6wg53x9h2XcFVFSdvr4V4RJSk2qcJZzcTo6lbbSeJajQpElfhY34aA8Z53jIOKfus4SCI2XsLDruNWxkdRCL1+/lUEnFGfUTYz2nOwHvR3IsnRNidJ0hFbQ06SvF6fH/1MQY6FV7/5GyCnZ7nyHYw0cb9hSzZEMBldVnniWkJcXUurB88nm8niUoB2nSV6oR2kZ7OPecBM49J6HWvsqqagqO1D3jaOH6vRyucZaQFHvyWkIcXdudOYSkZwkq0DTpK+WjCLeLtKRY0pJi+WEd+4tLT58l7PSaefRt/mEWr997xlmCxy2kJZ3sBGLsM4W4U0NHbaL0f1nlG/0XpFSAJcR4SEhNYEBq3WcJe4vLzjxDsM8Yvtl9mOLSM88S2sVF1ppp1MUePurYNlrPElSDNOkr5aAIt4suduIeXsf+4pIKdh86fQ3h5FlC9u7DLFy/lyqvs4RIt+uMawne1xG6tNOzBGXRfwVKBbGEWA8JsXWfJVRUVbP38OmzhJ0HT888WrfrEEfLzrx/QXJc5Bm/SfDuEOKjI4j1uPW3CWFAk75SIcrjdllJPDm2zv3FJRXsPHi81i+Y1+48xAfffE91HctuRbpdxES6ifG4iY10ExNp/Y22n8dGRtTe77H+xkRaHUdspJto+3WxHru+/RodfnKeJn2lWqmEWA/nxSZyXlpirX0VVdV8f9j6HUL+oVKOn6ikpLyKkvIqyiqqKCm3npeWV1FaUcXRskr2HzlBSUUlpeXVlJZXUlJRRVPXa4yKcFmdhedkh1K7Ezm9HVGjrncHVHtfdIQbl3YqDdKkr1QY8rhddEuOo1tyXLPfwxjDicpqSsurKKmosjoCu6OwnlfV3meXe3coJeWVHC4p5/vDNfZXVDU5ppOdQIynZidRz1nKqY4jolanc7LjOfk8KsLVKu77oElfKdUsIkK0x0qqSQF4/+pqq1MpqavDqKis0XHUf5ZSUl7FgWPllFaU2q+39p+orG44CC8u8epUvIeuztKJnNnpnNmJnFnfTaS7ZToVTfpKqaDkcsmpBFt7JSXfVVebU52C91mH95nGqU6ioooyu7yus5iCIxW1zmK8V3xtDLdLiPWcvh5yXloiz153gd/b7VPSF5FE4K/AAMAAtwCbgDeB7sAO4FpjzCGxurCZwDigBLjJGLPOl89XSqnmcrmEuKgI4gI0lbWyqrrWcNWp4a/yylr7vM9iSsurSE2KCUhcvrZ2JrDEGHO1iEQCscBvgJXGmBkiMh2YDjwIjAX62I8LgRftv0op1epEuF3Eu11Bt9ZSsyflikgCcDHwMoAxptwYcxiYALxmV3sNuMrengDMM5bVQKKIdG525EoppZrMl19i9AAKgVdE5GsR+auIxAEdjTF77ToFQEd7OxXY7fX6fLvsDCIyWUSyRCSrsLDQh/CUUkrV5EvSjwAGAy8aYy4AjmMN5ZxirLuuN2kmrzFmjjEm0xiTmZKS4kN4SimlavIl6ecD+caYL+3nb2N1AvtODtvYf/fb+/cAXbxen2aXKaWUaiHNTvrGmAJgt4j0s4suA3KABcAku2wS8L69vQC4USzDgGKvYSCllFItwNfZO3cDf7dn7mwDbsbqSN4SkVuBncC1dt1FWNM187CmbN7s42crpZRqIp+SvjEmG8isY9dlddQ1wFRfPk8ppZRvdB1VpZQKI2KaukxeCxKRQqwhouZqDxzwUzhOai3tAG1LsGotbWkt7QDf2tLNGFPn9MegTvq+EpEsY0xdw08hpbW0A7Qtwaq1tKW1tAMC1xYd3lFKqTCiSV8ppcJIa0/6c5wOwE9aSztA2xKsWktbWks7IEBtadVj+koppc7U2o/0lVJKedGkr5RSYSTkk76IjBGRTSKSZ9+0peb+KBF5097/pYh0b/koG6cRbblJRApFJNt+3OZEnA0Rkbkisl9ENtSzX0Rklt3Ob0VkcEvH2FiNaMulIlLs9Z38vqVjbAwR6SIiH4lIjoh8JyL31FEnJL6XRrYlVL6XaBFZIyLf2G35Yx11/JvDjDEh+wDcwFagJxAJfANk1KhzJzDb3p4IvOl03D605SbgOadjbURbLsZacXVDPfvHAYsBAYYBXzodsw9tuRT40Ok4G9GOzsBgezse2FzHv6+Q+F4a2ZZQ+V4EaGNve4AvgWE16vg1h4X6kf5QIM8Ys80YUw7Mx7pDlzfvO3m9DVwmLXHL+aZrTFtCgjHmE+DgWaqEzF3UGtGWkGCM2Wvse1IbY44CudS+iVFIfC+NbEtIsP9bH7OfeuxHzdk1fs1hoZ70G3M3rlN1jDGVQDGQ3CLRNU2j7iwG/Nw+9X5bRLrUsT8UNLatoeIi+/R8sYic63QwDbGHBy7AOqr0FnLfy1naAiHyvYiIW0Syse49stycvkfJSX7NYaGe9MPNB0B3Y8x5wHJO9/7KOeuw1jk5H3gWeM/heM5KRNoA7wC/MsYccToeXzTQlpD5XowxVcaYQVg3lhoqIgMC+XmhnvQbczeuU3VEJAJIAIpaJLqmabAtxpgiY8wJ++lfgSEtFJu/tZq7qBljjpw8PTfGLAI8ItLe4bDqJCIerCT5d2PMv+qoEjLfS0NtCaXv5SRjzGHgI2BMjV1+zWGhnvS/AvqISA/7Ri4Tse7Q5c37Tl5XA/829hWRINNgW2qMr/4UaywzFLWau6iJSKeT46siMhTr/6mgO6iwY3wZyDXG/KWeaiHxvTSmLSH0vaSISKK9HQOMBjbWqObXHObrnbMcZYypFJG7gKVYs1/mGmO+E5FHgSxjzAKsfxx/E5E8rAtyE52LuH6NbMs0EfkpUInVlpscC/gsROQNrNkT7UUkH3gE6wIVxpjZhNBd1BrRlquBO0SkEigFJgbpQcVw4AZgvT1+DPAboCuE3PfSmLaEyvfSGXhNRNzYdx00xnwYyBymyzAopVQYCfXhHaWUUk2gSV8ppcKIJn2llAojmvSVUiqMaNJXSqkwoklfKaXCiCZ9pZQKI/8feQy/LEGCCx8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the errors for each field below shows that the 'y' coordinates for each figure generally have the largest error and the errors growing rapidly approaching the tips of the fingers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error = labels_tips - preds\n",
    "sq_error = error * error\n",
    "avg_error = np.mean(sq_error, axis=0)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Prediction Error (mm)')\n",
    "#bar = plt.bar(df.columns[8:], avg_error)\n",
    "bar = plt.bar(df.columns[8:][[0,1,2,12,13,14,24,25,26,36,37,38,48,49,50,60,61,62]], avg_error)\n",
    "for i in range(0,18,3):\n",
    "    bar[i].set_color('coral')\n",
    "    bar[i+1].set_color('olivedrab')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc = tf.keras.models.Sequential()\n",
    "model_fc.add(LSTM(256, return_sequences=True, input_shape=(seq_length, 8)))\n",
    "model_fc.add(Dropout(0.5))\n",
    "model_fc.add(LSTM(256, return_sequences=True))\n",
    "model_fc.add(Dropout(0.5))\n",
    "model_fc.add(LSTM(128))\n",
    "model_fc.add(BatchNormalization())\n",
    "model_fc.add(Dense(512, input_dim=128))\n",
    "model_fc.add(Activation('relu'))\n",
    "model_fc.add(BatchNormalization())\n",
    "model_fc.add(Dropout(0.5))\n",
    "model_fc.add(Dense(512, input_dim=512))\n",
    "model_fc.add(Activation('relu'))\n",
    "model_fc.add(BatchNormalization())\n",
    "model_fc.add(Dropout(0.5))\n",
    "model_fc.add(Dense(256, input_dim=512))\n",
    "model_fc.add(Activation('relu'))\n",
    "model_fc.add(Dropout(0.3))\n",
    "model_fc.add(Dense(63, input_dim=64))\n",
    "model_fc.compile(optimizer='Adam', loss='mse')\n",
    "model_fc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_fc.fit(features, labels, batch_size=seq_le iiingth, epochs=4, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc.save('FC_model_jose_all_val_loss_400_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Fully Connected Layers\n",
    "Below is a test using a model with several dense layers after the LSTM layers, instead of using the pretrained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc = tf.keras.models.Sequential()\n",
    "model_fc.add(LSTM(64, return_sequences=True, input_shape=(seq_length, 8)))\n",
    "model_fc.add(Dropout(0.2))\n",
    "model_fc.add(LSTM(64))\n",
    "model_fc.add(BatchNormalization())\n",
    "model_fc.add(Dense(100, input_dim=64))\n",
    "model_fc.add(Activation('relu'))\n",
    "model_fc.add(BatchNormalization())\n",
    "model_fc.add(Dropout(0.2))\n",
    "model_fc.add(Dense(64, input_dim=64))\n",
    "model_fc.add(Activation('relu'))\n",
    "model_fc.add(Dropout(0.2))\n",
    "model_fc.add(Dense(63, input_dim=64))\n",
    "\n",
    "model_fc.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "history = model_fc.fit(features, labels, batch_size=seq_length, epochs=5, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc.save('FC_model_jose_all_val_loss_100_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Model Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_fc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-50db9fdb9c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_fc' is not defined"
     ]
    }
   ],
   "source": [
    "preds = model_fc.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = labels - preds\n",
    "sq_error = error * error\n",
    "avg_error = np.mean(sq_error, axis=0)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Prediction Error (mm)')\n",
    "bar = plt.bar(df.columns[8:], avg_error)\n",
    "for i in range(0,63,3):\n",
    "    bar[i].set_color('coral')\n",
    "    bar[i+1].set_color('olivedrab')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model On Tips Only\n",
    "In an attempt to improve the accuracy of prediction of the fingertips the above models are modified and trained only using the fingertip position data to understand if this simplification yields an improvement. This is done on both of the architectures tested above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 9 #dimensionality of 'feature' vector\n",
    "\n",
    "a_fn = None\n",
    "\n",
    "# Full autoencoder\n",
    "input_vec = Input(shape=(18,))\n",
    "dense_0 = Dense(32, activation=a_fn)(input_vec)\n",
    "dense_1 = Dense(16, activation=a_fn)(dense_0)\n",
    "encoded = Dense(encoding_dim, activation=a_fn)(dense_1)\n",
    "dense_2 = Dense(32, activation=a_fn, name='decoder_0')(encoded)\n",
    "dense_3 = Dense(16, activation=a_fn, name='decoder_1')(dense_2)\n",
    "decoded = Dense(18, activation=a_fn, name='decoder_output')(dense_3)\n",
    "\n",
    "autoencoder = Model(input_vec, decoded)\n",
    "\n",
    "# Encoder from autoencoder\n",
    "encoder = Model(input_vec, encoded)\n",
    "\n",
    "# Decoder from autoencoder layers\n",
    "decoder_input = Input(shape=(encoding_dim,), name='encoded_input')\n",
    "decode_0 = autoencoder.layers[-3](decoder_input)\n",
    "decode_1 = autoencoder.layers[-2](decode_0)\n",
    "decode_output = autoencoder.layers[-1](decode_1)\n",
    "decoder = Model(decoder_input, decode_output, name='decoder')\n",
    "\n",
    "# Train Autoencoder\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ar_t = label_ar[:,[0,1,2,12,13,14,24,25,26,36,37,38,48,49,50,60,61,62]]\n",
    "label_ar_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = autoencoder.fit(label_ar_t, label_ar_t, batch_size=512, epochs=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a smaller labels vector with only the tip and wrist x,y,z:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tip = labels[:,[0,1,2,12,13,14,24,25,26,36,37,38,48,49,50,60,61,62]]\n",
    "labels_tip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm layers\n",
    "inputs = Input(shape=(None, 8), name=\"inputs\")\n",
    "lstm_0 = LSTM(64, return_sequences=True, name=\"lstm_0\")(inputs)\n",
    "do = Dropout(0.2)(lstm_0)\n",
    "lstm_1 = LSTM(64, return_sequences=False, name=\"lstm_1\")(do)\n",
    "lstm_out = Dense(9, activation=None, name=\"lstm_out\")(lstm_1)\n",
    "\n",
    "#decoder layers\n",
    "decoder_0 = decoder.get_layer(\"decoder_0\")(lstm_out)\n",
    "decoder_0.trainable = False\n",
    "decoder_1 = decoder.get_layer(\"decoder_1\")(decoder_0)\n",
    "decoder_1.trainable = False\n",
    "decoder_output = decoder.get_layer(\"decoder_output\")(decoder_1)\n",
    "decoder_output.trainable = False\n",
    "\n",
    "model = Model(inputs, decoder_output, name=\"model_v1\")\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00, amsgrad=False)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(features, labels_tip, batch_size=seq_length, epochs=1, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Model Error\n",
    "Model error for the reduced Autoencoder architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = labels_tip - preds\n",
    "sq_error = error * error\n",
    "avg_error = np.mean(sq_error, axis=0)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Prediction Error (mm)')\n",
    "bar = plt.bar(df.columns[8:][[0,1,2,12,13,14,24,25,26,36,37,38,48,49,50,60,61,62]], avg_error)\n",
    "for i in range(0,18,3):\n",
    "    bar[i].set_color('coral')\n",
    "    bar[i+1].set_color('olivedrab')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips only with FC Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc = tf.keras.models.Sequential()\n",
    "model_fc.add(LSTM(64, return_sequences=True, input_shape=(seq_length, 8)))\n",
    "model_fc.add(Dropout(0.2))\n",
    "model_fc.add(LSTM(64))\n",
    "model_fc.add(BatchNormalization())\n",
    "model_fc.add(Dense(100, input_dim=64))\n",
    "model_fc.add(Activation('relu'))\n",
    "model_fc.add(BatchNormalization())\n",
    "model_fc.add(Dropout(0.2))\n",
    "model_fc.add(Dense(64, input_dim=64))\n",
    "model_fc.add(Activation('relu'))\n",
    "model_fc.add(Dropout(0.2))\n",
    "model_fc.add(Dense(18, input_dim=64))\n",
    "\n",
    "model_fc.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "history = model_fc.fit(features, labels_tip, batch_size=seq_length, epochs=9, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Model Error\n",
    "A similar visualizion is now done on this new more restricted model. These errors show 20% - 25% improvement on fingertip y position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_fc.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = labels_tip - preds\n",
    "sq_error = error * error\n",
    "avg_error = np.mean(sq_error, axis=0)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Prediction Error (mm)')\n",
    "bar = plt.bar(df.columns[8:][[0,1,2,12,13,14,24,25,26,36,37,38,48,49,50,60,61,62]], avg_error)\n",
    "for i in range(0,18,3):\n",
    "    bar[i].set_color('coral')\n",
    "    bar[i+1].set_color('olivedrab')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
